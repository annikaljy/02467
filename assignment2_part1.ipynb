{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 2\n",
    "### 02467 Computational Social Science Group 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Properties of the real-world network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load data\n",
    "papers_df = pd.read_csv(\"papers.csv\")\n",
    "author_df = pd.read_excel(\"author_data.csv\")\n",
    "\n",
    "# 2. Convert stringified lists into actual Python lists\n",
    "papers_df[\"author_ids\"] = papers_df[\"author_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "# 3. Create the real co-authorship network (G_real)\n",
    "G_real = nx.Graph()\n",
    "for row in papers_df.itertuples():\n",
    "    authors = row.author_ids\n",
    "    for i in range(len(authors)):\n",
    "        for j in range(i + 1, len(authors)):\n",
    "            G_real.add_edge(authors[i], authors[j])\n",
    "\n",
    "# 4. Compute real network stats\n",
    "N = G_real.number_of_nodes()\n",
    "L = G_real.number_of_edges()\n",
    "p = L / (N * (N - 1) / 2)\n",
    "avg_k = 2 * L / N\n",
    "\n",
    "print(f\"Real network: N = {N}, L = {L}, p = {p:.6f}, avg_k = = {avg_k:.2f}\")\n",
    "\n",
    "# 5. Visualize the real network \n",
    "degrees = dict(G_real.degree())\n",
    "node_sizes_real = [max(10, degrees[n] * 2) for n in G_real.nodes()]\n",
    "pos_real = nx.spring_layout(G_real, seed=42, iterations=20)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(G_real, pos_real, node_size=node_sizes_real, node_color='orange', alpha=0.8)\n",
    "nx.draw_networkx_edges(G_real, pos_real, edge_color='gray', width=0.8, alpha=0.3)\n",
    "plt.title(\"Real Co-authorship Network\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Generate random network using np.random.uniform\n",
    "def generate_random_network_with_uniform(N, p, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(N))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            if np.random.uniform(0, 1) < p:\n",
    "                G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "G_random = generate_random_network_with_uniform(N, p, seed=42)\n",
    "\n",
    "# 7. Visualize the random network\n",
    "degrees_rand = dict(G_random.degree())\n",
    "node_sizes_rand = [max(10, degrees_rand[n] * 2) for n in G_random.nodes()]\n",
    "pos_rand = nx.spring_layout(G_random, seed=42, iterations=20)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(G_random, pos_rand, node_size=node_sizes_rand, node_color='lightblue', alpha=0.8)\n",
    "nx.draw_networkx_edges(G_random, pos_rand, edge_color='gray', width=0.8, alpha=0.3)\n",
    "plt.title(\"Random Network\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What regime does your random network fall into? Is it above or below the critical threshold? \n",
    "The random network is above the critical threshold. The critical value for a giant component to appear is roughly $p_c = \\frac{1}{N}$, which in our case is about $0.0000658$. Since our $p \\approx 0.000419$, it’s well above that. So we’re in the regime where a large connected component is expected to exist.\n",
    "\n",
    "### 2. According to the textbook, what does the network’s structure resemble in this regime?\n",
    "In this regime, the network typically forms one big connected cluster along with some small isolated nodes. The degree distribution tends to follow a Poisson pattern, meaning most nodes have degrees close to the average, and high-degree nodes are rare. Overall, the structure looks pretty uniform and lacks distinct features.\n",
    "\n",
    "\n",
    "### 3. Based on your visualizations, identify the key differences between the actual and the random networks. Explain whether these differences are consistent with theoretical expectations.\n",
    "The real co-authorship network is much more clustered and uneven—it has hubs, visible communities, and a lot of local structure. The random network, by contrast, looks more uniform and spread out, without clear groupings. These differences line up with what theory predicts: real social networks often show high clustering and modularity, while random networks don’t capture those social patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get giant component from real network\n",
    "components_real = list(nx.connected_components(G_real))\n",
    "largest_cc_real = max(components_real, key=len)\n",
    "G_real_giant = G_real.subgraph(largest_cc_real).copy()\n",
    "\n",
    "# 2. Get giant component from random network\n",
    "components_rand = list(nx.connected_components(G_random))\n",
    "largest_cc_rand = max(components_rand, key=len)\n",
    "G_rand_giant = G_random.subgraph(largest_cc_rand).copy()\n",
    "\n",
    "# 3. Calculate average shortest path lengths\n",
    "avg_path_real = nx.average_shortest_path_length(G_real_giant)\n",
    "avg_path_rand = nx.average_shortest_path_length(G_rand_giant)\n",
    "\n",
    "print(f\"Avg shortest path (Real Network): {avg_path_real:.4f}\")\n",
    "print(f\"Avg shortest path (Random Network): {avg_path_rand:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Why do we consider the giant component only?\n",
    "\n",
    "Because average shortest path length is only defined between connected node pairs. If we include isolated components, the metric becomes meaningless (infinite distance). The giant component ensures that all node pairs are reachable.\n",
    "\n",
    "### 2. Why do we consider unweighted edges?\n",
    "\n",
    "The goal is to examine the basic topological structure of the network (small-world phenomenon), not edge weights or strengths. Unweighted paths help us measure pure connectivity.\n",
    "\n",
    "### 3. Does the Computational Social Scientists network exhibit the small-world phenomenon?\n",
    "\n",
    "Likely yes. If the real network shows a similar or slightly higher average shortest path compared to the random graph (with same N, L), and still shows higher clustering (can be computed if needed), it meets the criteria of a small-world network. These networks have short path lengths like random graphs but higher clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Words that characterize Computational Social Science communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What does TF stand for?\n",
    "#### Answer: TF stands for term frequency which measures how often a specific term appears in a set of text document. Higher frequency = higher relevance of word to document's content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 What does IDF stand for?\n",
    "#### Answer: IDF stands for inverse document frequency, reducing weight of common words and increasing weight of 'rarer' words. When a word appears in fewer text documents = more meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 8\n",
    "API_DELAY = 0.1\n",
    "\n",
    "authors_df = pd.read_csv('data/authors.csv')\n",
    "author_comm = pd.read_csv('data/author_communities.csv')\n",
    "abstracts = pd.read_csv('data/abstracts_with_collocations.csv')\n",
    "\n",
    "# Preprocess Tokens\n",
    "abstracts['tokens'] = abstracts['collocation_tokens'].map(\n",
    "    lambda x: eval(x) if isinstance(x, str) else x,\n",
    "    na_action='ignore'\n",
    ")\n",
    "work_to_tokens = {\n",
    "    work_id.split('/')[-1]: tokens  # Extract 'W...' from URLs\n",
    "    for work_id, tokens in zip(abstracts['id'], abstracts['tokens'])\n",
    "}\n",
    "\n",
    "# Parallel API Fetching\n",
    "def fetch_author_works(author_url):\n",
    "    try:\n",
    "        response = requests.get(f\"{author_url}?per-page=200\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            works = [w['id'].split('/')[-1] for w in response.json().get('results', [])]\n",
    "            return (author_url.split('/')[-1], works)  # Return (author_id, works)\n",
    "        return (author_url.split('/')[-1], None)\n",
    "    except Exception: return (author_url.split('/')[-1], None)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    results = list(tqdm(\n",
    "        executor.map(fetch_author_works, authors_df['Works API URL']),\n",
    "        total=len(authors_df)\n",
    "    ))\n",
    "\n",
    "author_to_works = {k: v for k, v in results if v is not None}\n",
    "failed_authors = [k for k, v in results if v is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community Processing\n",
    "valid_works = set(abstracts['id'])\n",
    "community_tokens = {}\n",
    "community_token_arrays = {} \n",
    "community_stats = {}\n",
    "\n",
    "for comm, group in author_comm.groupby('community'):\n",
    "    works = [\n",
    "        work \n",
    "        for author in group['author_id'] \n",
    "        for work in author_to_works.get(author, [])\n",
    "        if work in valid_works\n",
    "    ]\n",
    "    \n",
    "    comm_tokens = [\n",
    "        tok \n",
    "        for work in works \n",
    "        for tok in work_to_tokens.get(work, [])\n",
    "    ]\n",
    "    \n",
    "    community_tokens[comm] = comm_tokens\n",
    "\n",
    "# Save\n",
    "with open('community_tokens.json', 'w') as f:\n",
    "    json.dump(community_tokens, f)\n",
    "\n",
    "for community, group in tqdm(author_comm.groupby('community'), desc=\"Processing communities\"):\n",
    "    token_array = []  # store ALL tokens as a list of strings\n",
    "    stats = {\n",
    "        'authors_processed': 0,\n",
    "        'total_works': 0,\n",
    "        'works_with_tokens': 0\n",
    "    }\n",
    "    \n",
    "    for author_id in group['author_id']:\n",
    "        author_key = f\"works?filter=author.id:{author_id.split('/')[-1]}\"\n",
    "        \n",
    "        if author_key not in author_to_works: continue\n",
    "            \n",
    "        stats['authors_processed'] += 1\n",
    "        \n",
    "        for work_id in author_to_works[author_key]:\n",
    "            stats['total_works'] += 1\n",
    "            if work_id in work_to_tokens:\n",
    "                tokens = work_to_tokens[work_id]\n",
    "                if tokens:  # Only add non-empty\n",
    "                    token_array.extend(tokens)\n",
    "                    stats['works_with_tokens'] += 1\n",
    "    \n",
    "    community_token_arrays[community] = token_array\n",
    "    community_stats[community] = stats\n",
    "\n",
    "# Save\n",
    "with open('community_token_arrays.json', 'w') as f: \n",
    "    json.dump(community_token_arrays, f)\n",
    "\n",
    "print(\"\\nArray created and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate TF for each word and find the top 5 terms within the top 5 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    author_comm = pd.read_csv('data/author_communities.csv') \n",
    "    with open('community_token_arrays.json') as f:\n",
    "        community_token_arrays = json.load(f)\n",
    "except FileNotFoundError as e: raise\n",
    "\n",
    "community_sizes = author_comm['community'].value_counts().head(5)\n",
    "top_communities = community_sizes.index.astype(str).tolist()\n",
    "print(\"Top 5 communities by author count:\", top_communities)\n",
    "\n",
    "top_community_tf = {}\n",
    "for comm in top_communities:\n",
    "    tokens = community_token_arrays.get(comm, []) \n",
    "    if not tokens: continue\n",
    "        \n",
    "    tf = Counter(tokens)\n",
    "    total_terms = sum(tf.values())\n",
    "    top_terms = [\n",
    "        (term, count/total_terms) \n",
    "        for term, count in tf.most_common(5)\n",
    "    ]\n",
    "    top_community_tf[comm] = top_terms\n",
    "\n",
    "print(\"\\nTop Terms:\")\n",
    "for comm in top_communities:\n",
    "    if str(comm) not in top_community_tf: continue\n",
    "    term_line = \", \".join([f\"{term} ({freq:.3f}, {(freq*100):.1f}%)\" for term, freq in top_community_tf[str(comm)]])\n",
    "    \n",
    "    print(f\"\\nCommunity {comm} (Authors: {author_comm[author_comm['community']==int(comm)].shape[0]}): {term_line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, we calculate IDF for every word.\n",
    "\n",
    "author_comm = pd.read_csv('data/author_communities.csv')\n",
    "with open('community_token_arrays.json') as f: community_token_arrays = json.load(f)\n",
    "\n",
    "top_communities = author_comm['community'].value_counts().head(9).index.astype(str).tolist()\n",
    "\n",
    "documents = [\" \".join(community_token_arrays.get(comm, [])) for comm in top_communities]\n",
    "tf_results = {}\n",
    "for i, comm in enumerate(top_communities):\n",
    "    counter = Counter(documents[i].split())\n",
    "    total = sum(counter.values())\n",
    "    tf_results[comm] = [(word, count/total) for word, count in counter.most_common(10)]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_results = {}\n",
    "for i, comm in enumerate(top_communities):\n",
    "    feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "    tfidf_results[comm] = [(feature_names[idx], score) for idx, score in \n",
    "                         sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:10]]\n",
    "    \n",
    "top_authors = {}\n",
    "for comm in top_communities:\n",
    "    top_authors[comm] = (author_comm[author_comm['community'] == int(comm)]\n",
    "                        .sort_values('degree', ascending=False)\n",
    "                        .head(3)[['author_name', 'degree', 'author_id']]\n",
    "                        .values.tolist())\n",
    "\n",
    "for comm in top_communities:\n",
    "    print(f\"\\nCommunity {comm} ({author_comm[author_comm['community']==int(comm)].shape[0]} authors)\")\n",
    "    \n",
    "    # TF terms\n",
    "    tf_str = \" | \".join([f\"{word}:{score:.3f}\" for word, score in tf_results[comm]])\n",
    "    print(f\"Top 10 TF: {tf_str}\")\n",
    "    \n",
    "    # TF-IDF terms\n",
    "    tfidf_str = \" | \".join([f\"{word}:{score:.3f}\" for word, score in tfidf_results[comm]])\n",
    "    print(f\"Top 10 TF-IDF: {tfidf_str}\")\n",
    "    \n",
    "    # Top authors in one line\n",
    "    authors_str = \" | \".join([\n",
    "        f\"{name if pd.notna(name) else f'[{author_id}]'}:{degree}\" \n",
    "        for name, degree, author_id in top_authors[comm]\n",
    "    ])\n",
    "    print(f\"Top 3 Authors: {authors_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe similarities and differences between the communities.\n",
    "#### Similarities: most of the communities have word like data, information, and users. There are also common recurring themes related to human-computer interactions.\n",
    "#### Differences: The focus of each community appears to be quite different. E.g, Community 6 focuses on networks, community 17 focuses on social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why aren't the TFs not necessarily a good description of the communities?\n",
    "#### Answer: TFs does not account how some terms may be distinctive to a particular community. The words could be common in other communities as well, such as how \"data\" could appear in all communities but the \"data\" may be referring to different focuses and subbject matters. Moreover, rare but significant words can get drowned out by the higher frequency terms. Words like \"uncertainty\" would be more significant than words like \"data\", but the word \"data\" appears much more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What base logarithm did you use? Is that important?\n",
    "#### Answer: The base logarithm is base e. It is not that important as long as it is consistent with the other calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n",
    "#### Answer: Yes, the IDF makes 'rarer' words stand out more, such as 'reddit' (Community 20) and 'superspreaders' (Community 26) which gives more context as to what the tokens are about, as compared to the TF words which only gives generic words like 'people', 'humans' and 'information' which cannot really give much context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_comm = pd.read_csv('data/author_communities.csv')\n",
    "with open('community_token_arrays.json') as f:\n",
    "    community_token_arrays = json.load(f)\n",
    "\n",
    "top_communities = author_comm['community'].value_counts().head(9).index.astype(str).tolist()\n",
    "top_authors = {}\n",
    "for comm in top_communities:\n",
    "    top_authors[comm] = (author_comm[author_comm['community'] == int(comm)]\n",
    "                        .sort_values('degree', ascending=False)\n",
    "                        .head(3)[['author_name', 'degree', 'author_id']]\n",
    "                        .values.tolist())\n",
    "\n",
    "for comm in top_communities:\n",
    "    # Prepare text data\n",
    "    text = \" \".join(community_token_arrays.get(comm, []))\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=50,\n",
    "        contour_width=3,\n",
    "        contour_color='steelblue'\n",
    "    ).generate(text)\n",
    "    \n",
    "    authors_info = \"\\n\".join([\n",
    "        f\"{i+1}. {name if pd.notna(name) else f'[{author_id}]'} (degree: {degree})\"\n",
    "        for i, (name, degree, author_id) in enumerate(top_authors[comm])\n",
    "    ])\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Community {comm} Word Cloud\", fontsize=16, pad=20)\n",
    "    plt.figtext(\n",
    "        0.5, 0.05, \n",
    "        f\"Top Authors:\\n{authors_info}\",\n",
    "        ha=\"center\",\n",
    "        fontsize=12,\n",
    "        bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5}\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on your results. What can you conclude on the different sub-communities in Computational Social Science?\n",
    "#### Answer: The different sub-communities may all have different focuses but they are all related to people in some way, with the presence of words like \"human\", \"user\", \"individual\" and \"people\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up online the top author in each community. In light of your search, do your results make sense?\n",
    "#### Yes, each of the top author in the communities appear to be professors in well-established universities who are credible, which correlates to the higher degree in the community. Furthermore, the words in the wordcloud seems to agree with their field of study. For example, in community 8 wordcloud, the top author Stephan Lewandowsky is a psychologist and the words present in the wordcloud appears to be related - such as 'psychological', 'participant', 'research'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go back to Week 1, Exercise 1. Revise what you wrote on the topics in Computational Social Science. In light of your data-driven analysis, has your understanding of the field changed? How? (max 150 words)\n",
    "#### My understanding has changed slightly. Based on the results I have gotten, I believe Computational Social Science is highly relevant to humans. It is also not a single field of specific study, but rather a tapestry of different niches. Although there are some overlap (TF), the IDF terms shows the different focuses of each community. I also learnt that it is a highly interdisciplinary field,  as shown by the different words of IDFS - 'advertising', 'social media', 'cultural' and 'superspreaders' to name a few."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
