{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to repository: https://github.com/annikaljy/02467/\n",
    "\n",
    "## Contributions:\n",
    "### Byun, Jane - Part 1\n",
    "### Heo, Jiwon - Part 2\n",
    "### Law, Annika Jie Yu - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "048PsBWc_6tT"
   },
   "source": [
    "# Assigment 2\n",
    "### 02467 Computational Social Science Group 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLFDpLxh_6tT"
   },
   "source": [
    "## Part 1: Properties of the real-world network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU2gRYtd_6tU"
   },
   "source": [
    "1. Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_ZkJ89F_6tU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load data\n",
    "papers_df = pd.read_csv(\"papers.csv\")\n",
    "author_df = pd.read_excel(\"author_data.csv\")\n",
    "\n",
    "# 2. Convert stringified lists into actual Python lists\n",
    "papers_df[\"author_ids\"] = papers_df[\"author_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "# 3. Create the real co-authorship network (G_real)\n",
    "G_real = nx.Graph()\n",
    "for row in papers_df.itertuples():\n",
    "    authors = row.author_ids\n",
    "    for i in range(len(authors)):\n",
    "        for j in range(i + 1, len(authors)):\n",
    "            G_real.add_edge(authors[i], authors[j])\n",
    "\n",
    "# 4. Compute real network stats\n",
    "N = G_real.number_of_nodes()\n",
    "L = G_real.number_of_edges()\n",
    "p = L / (N * (N - 1) / 2)\n",
    "avg_k = 2 * L / N\n",
    "\n",
    "print(f\"Real network: N = {N}, L = {L}, p = {p:.6f}, avg_k = = {avg_k:.2f}\")\n",
    "\n",
    "# 5. Visualize the real network\n",
    "degrees = dict(G_real.degree())\n",
    "node_sizes_real = [max(10, degrees[n] * 2) for n in G_real.nodes()]\n",
    "pos_real = nx.spring_layout(G_real, seed=42, iterations=20)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(G_real, pos_real, node_size=node_sizes_real, node_color='orange', alpha=0.8)\n",
    "nx.draw_networkx_edges(G_real, pos_real, edge_color='gray', width=0.8, alpha=0.3)\n",
    "plt.title(\"Real Co-authorship Network\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Generate random network using np.random.uniform\n",
    "def generate_random_network_with_uniform(N, p, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(N))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            if np.random.uniform(0, 1) < p:\n",
    "                G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "G_random = generate_random_network_with_uniform(N, p, seed=42)\n",
    "\n",
    "# 7. Visualize the random network\n",
    "degrees_rand = dict(G_random.degree())\n",
    "node_sizes_rand = [max(10, degrees_rand[n] * 2) for n in G_random.nodes()]\n",
    "pos_rand = nx.spring_layout(G_random, seed=42, iterations=20)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(G_random, pos_rand, node_size=node_sizes_rand, node_color='lightblue', alpha=0.8)\n",
    "nx.draw_networkx_edges(G_random, pos_rand, edge_color='gray', width=0.8, alpha=0.3)\n",
    "plt.title(\"Random Network\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8mRafaT_6tU"
   },
   "source": [
    "### 1. What regime does your random network fall into? Is it above or below the critical threshold?\n",
    "The random network is above the critical threshold. The critical value for a giant component to appear is roughly $p_c = \\frac{1}{N}$, which in our case is about $0.0000658$. Since our $p \\approx 0.000419$, it’s well above that. So we’re in the regime where a large connected component is expected to exist.\n",
    "\n",
    "### 2. According to the textbook, what does the network’s structure resemble in this regime?\n",
    "In this regime, the network typically forms one big connected cluster along with some small isolated nodes. The degree distribution tends to follow a Poisson pattern, meaning most nodes have degrees close to the average, and high-degree nodes are rare. Overall, the structure looks pretty uniform and lacks distinct features.\n",
    "\n",
    "\n",
    "### 3. Based on your visualizations, identify the key differences between the actual and the random networks. Explain whether these differences are consistent with theoretical expectations.\n",
    "The real co-authorship network is much more clustered and uneven—it has hubs, visible communities, and a lot of local structure. The random network, by contrast, looks more uniform and spread out, without clear groupings. These differences line up with what theory predicts: real social networks often show high clustering and modularity, while random networks don’t capture those social patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4dvxvvn_6tV"
   },
   "outputs": [],
   "source": [
    "# 1. Get giant component from real network\n",
    "components_real = list(nx.connected_components(G_real))\n",
    "largest_cc_real = max(components_real, key=len)\n",
    "G_real_giant = G_real.subgraph(largest_cc_real).copy()\n",
    "\n",
    "# 2. Get giant component from random network\n",
    "components_rand = list(nx.connected_components(G_random))\n",
    "largest_cc_rand = max(components_rand, key=len)\n",
    "G_rand_giant = G_random.subgraph(largest_cc_rand).copy()\n",
    "\n",
    "# 3. Calculate average shortest path lengths\n",
    "avg_path_real = nx.average_shortest_path_length(G_real_giant)\n",
    "avg_path_rand = nx.average_shortest_path_length(G_rand_giant)\n",
    "\n",
    "print(f\"Avg shortest path (Real Network): {avg_path_real:.4f}\")\n",
    "print(f\"Avg shortest path (Random Network): {avg_path_rand:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TguLsreF_6tV"
   },
   "source": [
    "### 1. Why do we consider the giant component only?\n",
    "\n",
    "Because average shortest path length is only defined between connected node pairs. If we include isolated components, the metric becomes meaningless (infinite distance). The giant component ensures that all node pairs are reachable.\n",
    "\n",
    "### 2. Why do we consider unweighted edges?\n",
    "\n",
    "The goal is to examine the basic topological structure of the network (small-world phenomenon), not edge weights or strengths. Unweighted paths help us measure pure connectivity.\n",
    "\n",
    "### 3. Does the Computational Social Scientists network exhibit the small-world phenomenon?\n",
    "\n",
    "Likely yes. If the real network shows a similar or slightly higher average shortest path compared to the random graph (with same N, L), and still shows higher clustering (can be computed if needed), it meets the criteria of a small-world network. These networks have short path lengths like random graphs but higher clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdCuimM4ECUP"
   },
   "source": [
    "# Part 2 Network Analysis in Computational Social Science (Jiwon Heo)\n",
    "\n",
    "## Exercise 1: Mixing Patterns and Assortativity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yylxyc3rD7AL",
    "outputId": "f172a843-f03d-4479-9bee-732ae4dbb453"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VM2imIbkEJkP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data files\n",
    "authors_df = pd.read_csv('/content/drive/MyDrive/CSS/final_authors.csv')  # Author metadata\n",
    "papers_df = pd.read_csv('/content/drive/MyDrive/CSS/final_papers.csv')   # Paper-author relationships\n",
    "\n",
    "# Extract node attributes (country information)\n",
    "# Handle missing values: Replace missing country_code with 'Unknown'\n",
    "node_types = dict(zip(authors_df['id'], authors_df['country_code'].fillna('Unknown')))\n",
    "\n",
    "# Generate edge list (co-authorship relationships from papers)\n",
    "edges = []\n",
    "for paper_id, author_list in zip(papers_df['id'], papers_df['author_ids']):\n",
    "    # Convert string representation of list to actual list\n",
    "    authors = eval(author_list) if isinstance(author_list, str) else author_list\n",
    "    for i in range(len(authors)):\n",
    "        for j in range(i + 1, len(authors)):\n",
    "            edges.append((authors[i], authors[j]))  # Add co-authorship edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "931FjUmsEMVM",
    "outputId": "1eab3a33-9513-44a7-9b1f-b7a067593575"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_assortativity_coefficient(edges, node_types):\n",
    "    \"\"\"\n",
    "    Calculate the Assortativity Coefficient for a network.\n",
    "\n",
    "    Parameters:\n",
    "    - edges: List of tuples representing edges [(node1, node2), ...].\n",
    "    - node_types: Dictionary mapping nodes to their types {node: type}.\n",
    "\n",
    "    Returns:\n",
    "    - assortativity_coefficient: Assortativity Coefficient value (float).\n",
    "    \"\"\"\n",
    "    # Map types to indices\n",
    "    type_set = set(node_types.values())\n",
    "    type_indices = {t: i for i, t in enumerate(type_set)}\n",
    "    num_types = len(type_set)\n",
    "\n",
    "    # Create Mixing Matrix\n",
    "    e_matrix = np.zeros((num_types, num_types))\n",
    "    for node1, node2 in edges:\n",
    "        if node1 in node_types and node2 in node_types:  # Ensure nodes have types\n",
    "            type1 = type_indices[node_types[node1]]\n",
    "            type2 = type_indices[node_types[node2]]\n",
    "            e_matrix[type1, type2] += 1\n",
    "            e_matrix[type2, type1] += 1  # For undirected graph\n",
    "\n",
    "    e_matrix /= e_matrix.sum()  # Normalize to probabilities\n",
    "\n",
    "    # Calculate row and column sums\n",
    "    a = e_matrix.sum(axis=1)\n",
    "    b = e_matrix.sum(axis=0)\n",
    "\n",
    "    # Calculate Assortativity Coefficient\n",
    "    trace_e = np.trace(e_matrix)\n",
    "    sum_ab = np.sum(a * b)\n",
    "    assortativity_coefficient = (trace_e - sum_ab) / (1 - sum_ab)\n",
    "\n",
    "    return assortativity_coefficient\n",
    "\n",
    "# Assortativity Coefficient\n",
    "assortativity = calculate_assortativity_coefficient(edges, node_types)\n",
    "print(\"Assortativity Coefficient:\", assortativity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKpDaiin2rEa"
   },
   "source": [
    "#### 1. Calculate the Assortativity Coefficient for the network based on the country of each node. Implement the calculation using the formula provided during the lecture, also available in this paper (equation 2, here for directed networks). Do not use the NetworkX implementation.\n",
    "\n",
    "\n",
    "The assortativity coefficient for the network based on the country of each node has been calculated using the provided formula, yielding a value of approximately 0.3897. This indicates moderate assortativity, suggesting that nodes are somewhat more likely to connect with others from the same country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x91wUVdAXQnX"
   },
   "source": [
    "## Part 2: Configuration model\n",
    "In the following, we are going to assess the significance of the assortativity by comparing the network's assortativity coefficient against that of random networks generated through the configuration model.\n",
    "\n",
    "### 2. Implement the configuration model using the double edge swap algorithm to generate random networks. Ensure each node retains its original degree but with altered connections. Create a function that does that by following these steps:\n",
    "**a**. Create an exact copy of your original network.\n",
    "\n",
    "**b**. Select two edges, e1=(u,v) and e2=(x,y), ensuring u != y and v != x.\n",
    "\n",
    "**c**. Flip the direction of e1\n",
    " to e1=(v,u)\n",
    " 50% of the time. This ensure that your final results is not biased, in case your edges were sorted (they usually are).\n",
    "\n",
    "**d**. Ensure that new edges e′1=(e1[0],e2[1])\n",
    " and e′2=(e2[0],e1[1])\n",
    " do not already exist in the network.\n",
    "\n",
    "**e**. Remove edges e1\n",
    " and e2\n",
    " and add edges e′1\n",
    " and e′2\n",
    ".\n",
    "\n",
    "**f**. Repeat steps **b** to **e** until you have performed E*10 swaps, where E is the total number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4Om3aCyEpyS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load the actual data from the provided CSV files\n",
    "authors_df = pd.read_csv('/content/drive/MyDrive/CSS/final_authors.csv')\n",
    "papers_df = pd.read_csv('/content/drive/MyDrive/CSS/final_papers.csv')\n",
    "\n",
    "# Extract node types (author_id -> country_code)\n",
    "node_types = dict(zip(authors_df['id'], authors_df['country_code'].fillna('Unknown')))\n",
    "\n",
    "# Extract edges (co-authorship relationships)\n",
    "edges = []\n",
    "for _, row in papers_df.iterrows():\n",
    "    # Convert author_ids from string to list if necessary\n",
    "    author_ids = eval(row['author_ids']) if isinstance(row['author_ids'], str) else row['author_ids']\n",
    "    for i in range(len(author_ids)):\n",
    "        for j in range(i + 1, len(author_ids)):\n",
    "            edges.append((author_ids[i], author_ids[j]))  # Create undirected edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4m1BLLRIEuEP",
    "outputId": "5bf0400a-4774-4848-8914-21db22972c64"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "\n",
    "def double_edge_swap_preserve_degrees(graph, num_swaps):\n",
    "    \"\"\"Generate randomized network using double edge swap algorithm\"\"\"\n",
    "    G = graph.copy()\n",
    "    edges_list = list(G.edges())\n",
    "    swaps_done = 0\n",
    "    attempts = 0\n",
    "    max_attempts = num_swaps * 10\n",
    "\n",
    "    while swaps_done < num_swaps and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        # Select two distinct edges with non-overlapping nodes\n",
    "        e1_idx, e2_idx = random.sample(range(len(edges_list)), 2)\n",
    "        e1 = edges_list[e1_idx]\n",
    "        e2 = edges_list[e2_idx]\n",
    "\n",
    "        if len(set(e1 + e2)) < 4:\n",
    "            continue\n",
    "\n",
    "        # Randomly flip edge direction\n",
    "        if random.random() < 0.5:\n",
    "            e1 = (e1[1], e1[0])\n",
    "\n",
    "        # Create new edges\n",
    "        e1_new = (e1[0], e2[1])\n",
    "        e2_new = (e2[0], e1[1])\n",
    "\n",
    "        # Validate new edges\n",
    "        if (G.has_edge(*e1_new) or G.has_edge(*e2_new) or\n",
    "            e1_new[0] == e1_new[1] or e2_new[0] == e2_new[1]):\n",
    "            continue\n",
    "\n",
    "        # Update graph and edge list\n",
    "        G.remove_edges_from([e1, e2])\n",
    "        G.add_edges_from([e1_new, e2_new])\n",
    "\n",
    "        # Maintain edge list integrity\n",
    "        if e1_idx > e2_idx:\n",
    "            edges_list.pop(e1_idx)\n",
    "            edges_list.pop(e2_idx)\n",
    "        else:\n",
    "            edges_list.pop(e2_idx)\n",
    "            edges_list.pop(e1_idx)\n",
    "\n",
    "        edges_list.extend([e1_new, e2_new])\n",
    "        swaps_done += 1\n",
    "\n",
    "    return G\n",
    "\n",
    "# Load data and build graph\n",
    "authors_df = pd.read_csv('/content/drive/MyDrive/CSS/final_authors.csv')\n",
    "papers_df = pd.read_csv('/content/drive/MyDrive/CSS/final_papers.csv')\n",
    "node_types = dict(zip(authors_df['id'], authors_df['country_code'].fillna('Unknown')))\n",
    "\n",
    "# Construct collaboration network\n",
    "G = nx.Graph()\n",
    "for _, paper in papers_df.iterrows():\n",
    "    author_ids = eval(paper['author_ids']) if isinstance(paper['author_ids'], str) else paper['author_ids']\n",
    "    for i in range(len(author_ids)):\n",
    "        for j in range(i+1, len(author_ids)):\n",
    "            G.add_edge(author_ids[i], author_ids[j])\n",
    "\n",
    "# Configuration model implementation\n",
    "num_swaps = len(G.edges()) * 10\n",
    "G_random = double_edge_swap_preserve_degrees(G, num_swaps)\n",
    "\n",
    "# Validation outputs\n",
    "print(f\"Original network: {len(G.nodes)} nodes, {len(G.edges)} edges\")\n",
    "print(f\"Randomized network: {len(G_random.nodes)} nodes, {len(G_random.edges)} edges\")\n",
    "print(\"Degree preservation:\", dict(G.degree()) == dict(G_random.degree()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L4gLCQ03a_K"
   },
   "source": [
    "#### 2. Configuration Model Implementation\n",
    "The code successfully implements the configuration model using the double edge swap algorithm. It preserves the original degree of each node while randomizing the connections within the network. This is achieved by repeatedly swapping edges between randomly selected pairs, ensuring no self-loops or duplicate edges are introduced during the swaps. The function *double_edge_swap_preserve_degrees* performs these operations effectively.\n",
    "\n",
    "#### 3. Degree Preservation Validation\n",
    "The algorithm's correctness is confirmed as the degree distribution of nodes in the original network and the randomized network remains identical after edge swaps. This demonstrates that the configuration model accurately retains node degrees while altering network connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK_X0Z8dnOVp"
   },
   "source": [
    "### Part 3: Analyzing Assortativity in Random Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "BIA4jFUrm7Ym",
    "outputId": "f86a4217-607e-4811-ff9b-3495203f062a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def calculate_assortativity_by_country(graph, node_types):\n",
    "    \"\"\"Calculate country-based assortativity coefficient\"\"\"\n",
    "    nx.set_node_attributes(graph, node_types, \"country_code\")\n",
    "    return nx.attribute_assortativity_coefficient(graph, \"country_code\")\n",
    "\n",
    "# Original network validation\n",
    "original_degrees = dict(G.degree())\n",
    "swapped_degrees = dict(G_random.degree())\n",
    "print(\"Degree distribution consistency:\", original_degrees == swapped_degrees)\n",
    "\n",
    "# Calculate original network assortativity\n",
    "original_assort = calculate_assortativity_by_country(G, node_types)\n",
    "print(f\"Original network assortativity: {original_assort:.4f}\")\n",
    "\n",
    "# Generate and analyze 100 random networks\n",
    "num_simulations = 100\n",
    "random_assort = []\n",
    "progress_interval = max(1, num_simulations//10)  # Progress update interval\n",
    "\n",
    "print(f\"\\nGenerating {num_simulations} random networks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    # Create new randomized network\n",
    "    G_random = double_edge_swap_preserve_degrees(G, num_swaps)\n",
    "\n",
    "    # Calculate assortativity\n",
    "    random_assort.append(calculate_assortativity_by_country(G_random, node_types))\n",
    "\n",
    "    # Progress update\n",
    "    if (i+1) % progress_interval == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Completed {i+1}/{num_simulations} ({elapsed:.1f}s elapsed)\")\n",
    "\n",
    "# Analyze results\n",
    "mean_assort = np.mean(random_assort)\n",
    "std_assort = np.std(random_assort)\n",
    "z_score = (original_assort - mean_assort) / std_assort\n",
    "\n",
    "print(f\"\\nResults analysis:\")\n",
    "print(f\"- Random networks mean: {mean_assort:.4f}\")\n",
    "print(f\"- Random networks std: {std_assort:.4f}\")\n",
    "print(f\"- Original network Z-score: {z_score:.2f}\")\n",
    "\n",
    "# Visualization settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(random_assort, bins=20, alpha=0.7,\n",
    "                           label='Random Networks Distribution')\n",
    "plt.axvline(original_assort, color='r', linestyle='--',\n",
    "           label=f'Original Network ({original_assort:.4f})')\n",
    "plt.axvline(mean_assort, color='g', linestyle=':',\n",
    "           label=f'Random Mean ({mean_assort:.4f})')\n",
    "\n",
    "# 95% confidence interval\n",
    "ci_lower = mean_assort - 1.96*std_assort\n",
    "ci_upper = mean_assort + 1.96*std_assort\n",
    "plt.axvspan(ci_lower, ci_upper, color='gray', alpha=0.2,\n",
    "           label='95% Confidence Interval')\n",
    "\n",
    "plt.title('Assortativity Coefficient Distribution Comparison', fontsize=14)\n",
    "plt.xlabel('Assortativity Coefficient', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance determination\n",
    "if original_assort > ci_upper:\n",
    "    print(\"\\nConclusion: Original network shows significantly higher assortativity than random networks (p < 0.05)\")\n",
    "elif original_assort < ci_lower:\n",
    "    print(\"\\nConclusion: Original network shows significantly lower assortativity than random networks (p < 0.05)\")\n",
    "else:\n",
    "    print(\"\\nConclusion: No significant difference between original and random networks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iSgFb4YCm-3Z",
    "outputId": "bc0cc8d4-7a56-4853-fc00-00be44625b4f"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Ensure existing calculation results are available\n",
    "# (random_assort, original_assort variables assumed to exist)\n",
    "assert 'random_assort' in globals(), \"Existing execution results required\"\n",
    "assert 'original_assort' in globals(), \"Original data required\"\n",
    "\n",
    "# Style settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create plots for detailed analysis\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Full distribution\n",
    "ax[0].hist(random_assort, bins=50, alpha=0.5, color='blue',\n",
    "          edgecolor='black', density=True)\n",
    "sns.kdeplot(random_assort, color='red', ax=ax[0], linewidth=2)\n",
    "ax[0].axvline(original_assort, color='green', linestyle='--', linewidth=2)\n",
    "ax[0].set_title('Full Distribution Overview')\n",
    "ax[0].set_xlabel('Assortativity Coefficient')\n",
    "ax[0].set_ylabel('Density')\n",
    "\n",
    "# Zoomed-in region (around 0)\n",
    "zoom_range = (-0.05, 0.05)  # Adjust region of interest\n",
    "ax[1].hist(random_assort, bins=100, alpha=0.5, color='blue',\n",
    "         edgecolor='black', density=True, range=zoom_range)\n",
    "sns.kdeplot(random_assort, color='red', ax=ax[1], linewidth=2)\n",
    "ax[1].axvline(original_assort, color='green', linestyle='--', linewidth=2)\n",
    "ax[1].axvline(np.mean(random_assort), color='purple', linestyle=':', linewidth=2)\n",
    "ax[1].set_xlim(zoom_range)\n",
    "ax[1].set_title('Zoomed View (Zero-centered Region)')\n",
    "ax[1].set_xlabel('Assortativity Coefficient')\n",
    "ax[1].set_ylabel('Density')\n",
    "\n",
    "# Add statistical annotations\n",
    "stats_text = f\"\"\"Original: {original_assort:.4f}\n",
    "Random Mean: {np.mean(random_assort):.4f}\n",
    "95% CI: [{np.percentile(random_assort, 2.5):.4f}, {np.percentile(random_assort, 97.5):.4f}]\n",
    "Std Dev: {np.std(random_assort):.4f}\"\"\"\n",
    "ax[1].annotate(stats_text, xy=(0.65, 0.7), xycoords='axes fraction',\n",
    "              bbox=dict(boxstyle=\"round\", fc=\"white\"))\n",
    "\n",
    "# Create legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='blue', alpha=0.5, label='Random Networks'),\n",
    "    plt.Line2D([0], [0], color='red', lw=2, label='KDE Density'),\n",
    "    plt.Line2D([0], [0], color='green', linestyle='--', lw=2, label='Original Network'),\n",
    "    plt.Line2D([0], [0], color='purple', linestyle=':', lw=2, label='Random Mean')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for additional analysis\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=random_assort, width=0.3, color='skyblue')\n",
    "plt.axvline(original_assort, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Detailed Spread Analysis')\n",
    "plt.xlabel('Assortativity Coefficient')\n",
    "plt.xlim(zoom_range)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Percentile calculation\n",
    "percentile = 100 * sum(a < original_assort for a in random_assort)/len(random_assort)\n",
    "print(f\"\\n[Statistical Details]\")\n",
    "print(f\"Original value percentile in random distribution: {percentile:.1f}%\")\n",
    "print(f\"Original - Random Mean: {original_assort - np.mean(random_assort):.4f}\")\n",
    "print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-tpWxmJ4ZkV"
   },
   "source": [
    "#### 4. Generate and analyze at least 100 random networks using the configuration model. For each, calculate the assortativity with respect to the country and plot the distribution of these values. Compare the results with the assortativity of your original network to determine if connections within the same country are significantly higher than chance.\n",
    "\n",
    "\n",
    "The original network's assortativity coefficient with respect to the country is 0.3413, significantly higher than the mean assortativity coefficient of 0.0003 calculated across 100 randomized networks generated using the configuration model. This indicates that connections within the same country are substantially higher than would be expected by chance, as confirmed by a Z-score of 175.74 and a percentile analysis showing the original network's value lies far above the random distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk2iHxb5nEUn"
   },
   "source": [
    "## Exercise 2: Zachary's karate club\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nxUieK_mXby5",
    "outputId": "a86d35dd-bd76-4be3-bec9-771deeb1a5cc"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from community import community_louvain\n",
    "\n",
    "# Load Zachary's Karate Club graph\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Visualize the graph with node colors based on club split\n",
    "node_colors = ['blue' if G.nodes[node]['club'] == 'Mr. Hi' else 'red' for node in G.nodes]\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, with_labels=True, node_color=node_colors, node_size=500, font_color='white')\n",
    "plt.title(\"Zachary's Karate Club Graph Visualization\")\n",
    "plt.show()\n",
    "\n",
    "# Function to compute modularity of a graph partitioning\n",
    "def modularity(graph, partition):\n",
    "    m = graph.size(weight='weight')  # Total edge weight\n",
    "    degrees = dict(graph.degree(weight='weight'))\n",
    "    modularity_value = 0\n",
    "\n",
    "    for community in partition:\n",
    "        for u in community:\n",
    "            for v in community:\n",
    "                if graph.has_edge(u, v):\n",
    "                    modularity_value += graph[u][v].get('weight', 1) - (degrees[u] * degrees[v]) / (2 * m)\n",
    "\n",
    "    return modularity_value / (2 * m)\n",
    "\n",
    "# Compute modularity for the club split partitioning\n",
    "club_partition = {\n",
    "    'Mr. Hi': [node for node in G.nodes if G.nodes[node]['club'] == 'Mr. Hi'],\n",
    "    'Officer': [node for node in G.nodes if G.nodes[node]['club'] == 'Officer']\n",
    "}\n",
    "club_modularity = modularity(G, club_partition.values())\n",
    "print(f\"Club Split Modularity: {club_modularity:.4f}\")\n",
    "\n",
    "# Create 1000 randomized versions of the Karate Club network and compute modularity\n",
    "num_randomizations = 1000\n",
    "random_modularities = []\n",
    "\n",
    "for _ in range(num_randomizations):\n",
    "    G_random = nx.double_edge_swap(G.copy(), nswap=10 * G.number_of_edges(), max_tries=100 * G.number_of_edges())\n",
    "    random_modularity = modularity(G_random, club_partition.values())\n",
    "    random_modularities.append(random_modularity)\n",
    "\n",
    "# Compute average and standard deviation of modularity for random networks\n",
    "mean_random_modularity = np.mean(random_modularities)\n",
    "std_random_modularity = np.std(random_modularities)\n",
    "print(f\"Random Networks Modularity - Mean: {mean_random_modularity:.4f}, Std Dev: {std_random_modularity:.4f}\")\n",
    "\n",
    "# Plot distribution of random modularity vs actual modularity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(random_modularities, bins=30, alpha=0.7, label='Random Networks Modularity')\n",
    "plt.axvline(club_modularity, color='red', linestyle='--', linewidth=2, label=f'Actual Club Modularity ({club_modularity:.4f})')\n",
    "plt.title('Distribution of Random Modularity vs Actual Modularity', fontsize=14)\n",
    "plt.xlabel('Modularity', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Use Louvain algorithm to find communities and compute its modularity\n",
    "louvain_partition = community_louvain.best_partition(G)\n",
    "louvain_communities = {}\n",
    "for node, community_id in louvain_partition.items():\n",
    "    if community_id not in louvain_communities:\n",
    "        louvain_communities[community_id] = []\n",
    "    louvain_communities[community_id].append(node)\n",
    "\n",
    "louvain_modularity = modularity(G, louvain_communities.values())\n",
    "print(f\"Louvain Partition Modularity: {louvain_modularity:.4f}\")\n",
    "\n",
    "# Create confusion matrix to compare Louvain communities with club split\n",
    "club_labels = {node: 0 if G.nodes[node]['club'] == 'Mr. Hi' else 1 for node in G.nodes}\n",
    "num_louvain_communities = len(louvain_communities)\n",
    "confusion_matrix = np.zeros((num_louvain_communities, 2), dtype=int)\n",
    "\n",
    "for community_id, nodes in louvain_communities.items():\n",
    "    for node in nodes:\n",
    "        club_label = club_labels[node]\n",
    "        confusion_matrix[community_id, club_label] += 1\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30PcddQd5jwe"
   },
   "source": [
    "#### 1. Graph Visualization\n",
    "The graph of Zachary's karate club is visualized with nodes colored based on the club split attribute. Nodes belonging to one group are marked in red, while those in the other group are marked in blue. This visualization highlights the community structure within the network.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Modularity Function Implementation\n",
    "A custom function is implemented to calculate the modularity of a graph partitioning using Equation 9.12 from the textbook. The function computes modularity by comparing the actual edge density within partitions to the expected density under a random configuration model, ensuring each node retains its degree.\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Concept of Modularity\n",
    "Modularity measures the strength of division of a network into communities. It quantifies how well-connected nodes within the same community are compared to what would be expected in a random network with similar degree distribution. Higher modularity indicates stronger community structure.\n",
    "\n",
    "\n",
    "#### 4. Modularity of Karate Club Split Partitioning\n",
    "The modularity of the Karate club split partitioning, computed using the custom function, is approximately 0.3413. This value reflects a strong community structure, indicating that nodes within the same club are well-connected compared to connections across different clubs.\n",
    "\n",
    "#### 5. Modularity Calculation for 1000 Randomized Networks\n",
    "Using the double edge swap algorithm, 1000 randomized versions of the Karate Club network were generated. For each randomized network, the modularity of the \"club\" split was calculated and stored in a list for further analysis.\n",
    "\n",
    "#### 6. Average and Standard Deviation of Random Network Modularity\n",
    "The modularity values for the randomized networks have a mean of 0.0003 and a standard deviation of 0.0019, significantly lower than the original network's modularity. This comparison highlights that the original network's community structure is far stronger than would occur by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fbVYmFKpmQb",
    "outputId": "f71821f8-79cf-4e7e-8f9d-c08833e42cfa"
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import girvan_newman\n",
    "\n",
    "# Apply Girvan-Newman algorithm\n",
    "gn_hierarchy = girvan_newman(G)\n",
    "gn_partition = next(gn_hierarchy)  # First split of communities\n",
    "gn_communities = [list(community) for community in gn_partition]\n",
    "\n",
    "# Compute modularity for Girvan-Newman partition\n",
    "gn_modularity = modularity(G, gn_communities)\n",
    "print(f\"Girvan-Newman Partition Modularity: {gn_modularity:.4f}\")\n",
    "\n",
    "# Compare Girvan-Newman communities with Louvain communities\n",
    "print(\"Girvan-Newman Communities:\", gn_communities)\n",
    "print(\"Louvain Communities:\", list(louvain_communities.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqlguf5UpoGg",
    "outputId": "97e3ecc2-f5aa-44cd-fbda-60cfc6b5d2ea"
   },
   "outputs": [],
   "source": [
    "# Compute centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Display top influential nodes based on each metric\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"Top Nodes by Degree Centrality:\", top_degree)\n",
    "print(\"Top Nodes by Betweenness Centrality:\", top_betweenness)\n",
    "print(\"Top Nodes by Closeness Centrality:\", top_closeness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d21dO__jpqOa",
    "outputId": "669806a5-fb34-4d99-82cd-a90898ceebe1"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Create adjacency matrix for spectral clustering\n",
    "adj_matrix = nx.to_numpy_array(G)\n",
    "\n",
    "# Apply spectral clustering with 2 clusters (expected split)\n",
    "spectral_clustering = SpectralClustering(n_clusters=2, affinity='precomputed', random_state=42)\n",
    "spectral_labels = spectral_clustering.fit_predict(adj_matrix)\n",
    "\n",
    "# Assign nodes to predicted groups\n",
    "spectral_groups = {i: [] for i in range(2)}\n",
    "for node, label in enumerate(spectral_labels):\n",
    "    spectral_groups[label].append(node)\n",
    "\n",
    "print(\"Spectral Clustering Predicted Groups:\", spectral_groups)\n",
    "print(f\"Node 9 belongs to Group {spectral_labels[9]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "N64vOoPcpsED",
    "outputId": "6c454c50-4514-47d4-d7f6-ac0432cecac5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assign colors for visualization\n",
    "colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "louvain_colors = {node: colors[louvain_partition[node] % len(colors)] for node in G.nodes}\n",
    "gn_colors = {node: colors[int(any(node in community for community in gn_communities))] for node in G.nodes}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Visualize Louvain communities\n",
    "plt.subplot(1, 2, 1)\n",
    "nx.draw(G, with_labels=True, node_color=list(louvain_colors.values()), node_size=500, font_color=\"white\")\n",
    "plt.title(\"Louvain Communities\")\n",
    "\n",
    "# Visualize Girvan-Newman communities\n",
    "plt.subplot(1, 2, 2)\n",
    "nx.draw(G, with_labels=True, node_color=list(gn_colors.values()), node_size=500, font_color=\"white\")\n",
    "plt.title(\"Girvan-Newman Communities\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp7Pb9hH8qSI"
   },
   "source": [
    "####7. Distribution of Random Modularity and Comparison with Actual Modularity\n",
    "The distribution of modularity values for 1000 randomized versions of the Karate Club network was plotted as a histogram. The actual modularity of the club split, 0.6261, was marked as a vertical green dashed line on the plot. This visualization demonstrates that the original network's modularity is significantly higher than the modularity values observed in the randomized networks, confirming a stronger community structure in the original network compared to random chance.\n",
    "\n",
    "####8. Comment on the Figure\n",
    "The club split is a reasonably good partitioning, as evidenced by its modularity value being significantly higher than those observed in randomized networks. The randomization experiment was conducted to establish a baseline for comparison, demonstrating that the original network's modularity reflects genuine community structure rather than random chance. Preserving node degrees ensures that the randomized networks maintain the same connectivity distribution, isolating modularity differences to the structure of connections rather than degree variations.\n",
    "\n",
    "\n",
    "####9. Modularity of Louvain Algorithm Communities\n",
    "Using the Python Louvain algorithm implementation, the modularity of the detected communities was calculated as **0.6261**, which is higher than the modularity of the club split partitioning. This comparison reveals that the Louvain algorithm identifies community structures that are more strongly connected internally than those defined by the club split.\n",
    "\n",
    "####10. Confusion Matrix Analysis\n",
    "The confusion matrix comparing Louvain communities with the club split partitioning highlights the correspondence between detected communities and predefined groups. While some overlap exists, discrepancies indicate that the Louvain algorithm captures additional or alternative community structures beyond those defined by the club split. This suggests that community detection algorithms can reveal nuanced relationships not immediately evident from predefined partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPR_GgQc_6tV"
   },
   "source": [
    "## Part 3 - Words that characterize Computational Social Science communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAvQHjKx_6tV"
   },
   "source": [
    "### 1.1 What does TF stand for?\n",
    "#### Answer: TF stands for term frequency which measures how often a specific term appears in a set of text document. Higher frequency = higher relevance of word to document's content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8ffrcVN_6tV"
   },
   "source": [
    "### 1.2 What does IDF stand for?\n",
    "#### Answer: IDF stands for inverse document frequency, reducing weight of common words and increasing weight of 'rarer' words. When a word appears in fewer text documents = more meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B18o7ckZ_6tV"
   },
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5khe17r8_6tV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_Em-iFS_6tV",
    "outputId": "2ccb17fa-b61c-4c4f-a404-abdbe96efe31"
   },
   "outputs": [],
   "source": [
    "MAX_WORKERS = 8\n",
    "API_DELAY = 0.1\n",
    "\n",
    "authors_df = pd.read_csv('data/authors.csv')\n",
    "author_comm = pd.read_csv('data/author_communities.csv')\n",
    "abstracts = pd.read_csv('data/abstracts_with_collocations.csv')\n",
    "\n",
    "# Preprocess Tokens\n",
    "abstracts['tokens'] = abstracts['collocation_tokens'].map(\n",
    "    lambda x: eval(x) if isinstance(x, str) else x,\n",
    "    na_action='ignore'\n",
    ")\n",
    "work_to_tokens = {\n",
    "    work_id.split('/')[-1]: tokens  # Extract 'W...' from URLs\n",
    "    for work_id, tokens in zip(abstracts['id'], abstracts['tokens'])\n",
    "}\n",
    "\n",
    "# Parallel API Fetching\n",
    "def fetch_author_works(author_url):\n",
    "    try:\n",
    "        response = requests.get(f\"{author_url}?per-page=200\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            works = [w['id'].split('/')[-1] for w in response.json().get('results', [])]\n",
    "            return (author_url.split('/')[-1], works)  # Return (author_id, works)\n",
    "        return (author_url.split('/')[-1], None)\n",
    "    except Exception: return (author_url.split('/')[-1], None)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    results = list(tqdm(\n",
    "        executor.map(fetch_author_works, authors_df['Works API URL']),\n",
    "        total=len(authors_df)\n",
    "    ))\n",
    "\n",
    "author_to_works = {k: v for k, v in results if v is not None}\n",
    "failed_authors = [k for k, v in results if v is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCU2DONk_6tW",
    "outputId": "85b2c75a-7cd4-4c05-87c5-a7d571302538"
   },
   "outputs": [],
   "source": [
    "# Community Processing\n",
    "valid_works = set(abstracts['id'])\n",
    "community_tokens = {}\n",
    "community_token_arrays = {}\n",
    "community_stats = {}\n",
    "\n",
    "for comm, group in author_comm.groupby('community'):\n",
    "    works = [\n",
    "        work\n",
    "        for author in group['author_id']\n",
    "        for work in author_to_works.get(author, [])\n",
    "        if work in valid_works\n",
    "    ]\n",
    "\n",
    "    comm_tokens = [\n",
    "        tok\n",
    "        for work in works\n",
    "        for tok in work_to_tokens.get(work, [])\n",
    "    ]\n",
    "\n",
    "    community_tokens[comm] = comm_tokens\n",
    "\n",
    "# Save\n",
    "with open('community_tokens.json', 'w') as f:\n",
    "    json.dump(community_tokens, f)\n",
    "\n",
    "for community, group in tqdm(author_comm.groupby('community'), desc=\"Processing communities\"):\n",
    "    token_array = []  # store ALL tokens as a list of strings\n",
    "    stats = {\n",
    "        'authors_processed': 0,\n",
    "        'total_works': 0,\n",
    "        'works_with_tokens': 0\n",
    "    }\n",
    "\n",
    "    for author_id in group['author_id']:\n",
    "        author_key = f\"works?filter=author.id:{author_id.split('/')[-1]}\"\n",
    "\n",
    "        if author_key not in author_to_works: continue\n",
    "\n",
    "        stats['authors_processed'] += 1\n",
    "\n",
    "        for work_id in author_to_works[author_key]:\n",
    "            stats['total_works'] += 1\n",
    "            if work_id in work_to_tokens:\n",
    "                tokens = work_to_tokens[work_id]\n",
    "                if tokens:  # Only add non-empty\n",
    "                    token_array.extend(tokens)\n",
    "                    stats['works_with_tokens'] += 1\n",
    "\n",
    "    community_token_arrays[community] = token_array\n",
    "    community_stats[community] = stats\n",
    "\n",
    "# Save\n",
    "with open('community_token_arrays.json', 'w') as f:\n",
    "    json.dump(community_token_arrays, f)\n",
    "\n",
    "print(\"\\nArray created and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DuePV7f_6tW"
   },
   "source": [
    "### 3. Calculate TF for each word and find the top 5 terms within the top 5 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9iqxEbx_6tW",
    "outputId": "e6fa4507-5783-436e-b4fc-3fe3563a7103"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    author_comm = pd.read_csv('data/author_communities.csv')\n",
    "    with open('community_token_arrays.json') as f:\n",
    "        community_token_arrays = json.load(f)\n",
    "except FileNotFoundError as e: raise\n",
    "\n",
    "community_sizes = author_comm['community'].value_counts().head(5)\n",
    "top_communities = community_sizes.index.astype(str).tolist()\n",
    "print(\"Top 5 communities by author count:\", top_communities)\n",
    "\n",
    "top_community_tf = {}\n",
    "for comm in top_communities:\n",
    "    tokens = community_token_arrays.get(comm, [])\n",
    "    if not tokens: continue\n",
    "\n",
    "    tf = Counter(tokens)\n",
    "    total_terms = sum(tf.values())\n",
    "    top_terms = [\n",
    "        (term, count/total_terms)\n",
    "        for term, count in tf.most_common(5)\n",
    "    ]\n",
    "    top_community_tf[comm] = top_terms\n",
    "\n",
    "print(\"\\nTop Terms:\")\n",
    "for comm in top_communities:\n",
    "    if str(comm) not in top_community_tf: continue\n",
    "    term_line = \", \".join([f\"{term} ({freq:.3f}, {(freq*100):.1f}%)\" for term, freq in top_community_tf[str(comm)]])\n",
    "\n",
    "    print(f\"\\nCommunity {comm} (Authors: {author_comm[author_comm['community']==int(comm)].shape[0]}): {term_line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGK50Din_6tW",
    "outputId": "2ec911cf-8056-46d9-8b2b-373272b00456"
   },
   "outputs": [],
   "source": [
    "## Next, we calculate IDF for every word.\n",
    "\n",
    "author_comm = pd.read_csv('data/author_communities.csv')\n",
    "with open('community_token_arrays.json') as f: community_token_arrays = json.load(f)\n",
    "\n",
    "top_communities = author_comm['community'].value_counts().head(9).index.astype(str).tolist()\n",
    "\n",
    "documents = [\" \".join(community_token_arrays.get(comm, [])) for comm in top_communities]\n",
    "tf_results = {}\n",
    "for i, comm in enumerate(top_communities):\n",
    "    counter = Counter(documents[i].split())\n",
    "    total = sum(counter.values())\n",
    "    tf_results[comm] = [(word, count/total) for word, count in counter.most_common(10)]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_results = {}\n",
    "for i, comm in enumerate(top_communities):\n",
    "    feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "    tfidf_results[comm] = [(feature_names[idx], score) for idx, score in\n",
    "                         sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:10]]\n",
    "\n",
    "top_authors = {}\n",
    "for comm in top_communities:\n",
    "    top_authors[comm] = (author_comm[author_comm['community'] == int(comm)]\n",
    "                        .sort_values('degree', ascending=False)\n",
    "                        .head(3)[['author_name', 'degree', 'author_id']]\n",
    "                        .values.tolist())\n",
    "\n",
    "for comm in top_communities:\n",
    "    print(f\"\\nCommunity {comm} ({author_comm[author_comm['community']==int(comm)].shape[0]} authors)\")\n",
    "\n",
    "    # TF terms\n",
    "    tf_str = \" | \".join([f\"{word}:{score:.3f}\" for word, score in tf_results[comm]])\n",
    "    print(f\"Top 10 TF: {tf_str}\")\n",
    "\n",
    "    # TF-IDF terms\n",
    "    tfidf_str = \" | \".join([f\"{word}:{score:.3f}\" for word, score in tfidf_results[comm]])\n",
    "    print(f\"Top 10 TF-IDF: {tfidf_str}\")\n",
    "\n",
    "    # Top authors in one line\n",
    "    authors_str = \" | \".join([\n",
    "        f\"{name if pd.notna(name) else f'[{author_id}]'}:{degree}\"\n",
    "        for name, degree, author_id in top_authors[comm]\n",
    "    ])\n",
    "    print(f\"Top 3 Authors: {authors_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1-mFvii_6tW"
   },
   "source": [
    "### Describe similarities and differences between the communities.\n",
    "#### Similarities: most of the communities have word like data, information, and users. There are also common recurring themes related to human-computer interactions.\n",
    "#### Differences: The focus of each community appears to be quite different. E.g, Community 6 focuses on networks, community 17 focuses on social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pFNvIdY_6tW"
   },
   "source": [
    "### Why aren't the TFs not necessarily a good description of the communities?\n",
    "#### Answer: TFs does not account how some terms may be distinctive to a particular community. The words could be common in other communities as well, such as how \"data\" could appear in all communities but the \"data\" may be referring to different focuses and subbject matters. Moreover, rare but significant words can get drowned out by the higher frequency terms. Words like \"uncertainty\" would be more significant than words like \"data\", but the word \"data\" appears much more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LkGoA1D_6tW"
   },
   "source": [
    "### What base logarithm did you use? Is that important?\n",
    "#### Answer: The base logarithm is base e. It is not that important as long as it is consistent with the other calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJOkH5hb_6tW"
   },
   "source": [
    "### Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n",
    "#### Answer: Yes, the IDF makes 'rarer' words stand out more, such as 'reddit' (Community 20) and 'superspreaders' (Community 26) which gives more context as to what the tokens are about, as compared to the TF words which only gives generic words like 'people', 'humans' and 'information' which cannot really give much context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mut93nbg_6tX",
    "outputId": "7f5046a6-60ff-4dc9-e557-1c52dcaffe81"
   },
   "outputs": [],
   "source": [
    "author_comm = pd.read_csv('data/author_communities.csv')\n",
    "with open('community_token_arrays.json') as f:\n",
    "    community_token_arrays = json.load(f)\n",
    "\n",
    "top_communities = author_comm['community'].value_counts().head(9).index.astype(str).tolist()\n",
    "top_authors = {}\n",
    "for comm in top_communities:\n",
    "    top_authors[comm] = (author_comm[author_comm['community'] == int(comm)]\n",
    "                        .sort_values('degree', ascending=False)\n",
    "                        .head(3)[['author_name', 'degree', 'author_id']]\n",
    "                        .values.tolist())\n",
    "\n",
    "for comm in top_communities:\n",
    "    # Prepare text data\n",
    "    text = \" \".join(community_token_arrays.get(comm, []))\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=50,\n",
    "        contour_width=3,\n",
    "        contour_color='steelblue'\n",
    "    ).generate(text)\n",
    "\n",
    "    authors_info = \"\\n\".join([\n",
    "        f\"{i+1}. {name if pd.notna(name) else f'[{author_id}]'} (degree: {degree})\"\n",
    "        for i, (name, degree, author_id) in enumerate(top_authors[comm])\n",
    "    ])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Community {comm} Word Cloud\", fontsize=16, pad=20)\n",
    "    plt.figtext(\n",
    "        0.5, 0.05,\n",
    "        f\"Top Authors:\\n{authors_info}\",\n",
    "        ha=\"center\",\n",
    "        fontsize=12,\n",
    "        bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5}\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TC7laYg_6tX"
   },
   "source": [
    "### Comment on your results. What can you conclude on the different sub-communities in Computational Social Science?\n",
    "#### Answer: The different sub-communities may all have different focuses but they are all related to people in some way, with the presence of words like \"human\", \"user\", \"individual\" and \"people\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmUK3RlB_6tX"
   },
   "source": [
    "### Look up online the top author in each community. In light of your search, do your results make sense?\n",
    "#### Yes, each of the top author in the communities appear to be professors in well-established universities who are credible, which correlates to the higher degree in the community. Furthermore, the words in the wordcloud seems to agree with their field of study. For example, in community 8 wordcloud, the top author Stephan Lewandowsky is a psychologist and the words present in the wordcloud appears to be related - such as 'psychological', 'participant', 'research'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptf6YXgC_6tX"
   },
   "source": [
    "### Go back to Week 1, Exercise 1. Revise what you wrote on the topics in Computational Social Science. In light of your data-driven analysis, has your understanding of the field changed? How? (max 150 words)\n",
    "#### My understanding has changed slightly. Based on the results I have gotten, I believe Computational Social Science is highly relevant to humans. It is also not a single field of specific study, but rather a tapestry of different niches. Although there are some overlap (TF), the IDF terms shows the different focuses of each community. I also learnt that it is a highly interdisciplinary field,  as shown by the different words of IDFS - 'advertising', 'social media', 'cultural' and 'superspreaders' to name a few."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
