{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmE8x6ZI0URp"
      },
      "source": [
        "# Assigment 1\n",
        "### 02467 Computational Social Science Group 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5nPdY790URt"
      },
      "source": [
        "## Part 1: Web-scraping\n",
        "### _Exercise: Web-scraping the list of participants to the International Conference in Computational Social Science_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAwKhXU30URt"
      },
      "outputs": [],
      "source": [
        "# Week 1 - Web Scraping Q1\n",
        "\n",
        "from google.colab import drive\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def clean_name(name):\n",
        "    # Clean name - remove special characters and unnecessary whitespace\n",
        "    name = re.sub(r'[,\\n\\t\\r]', '', name)\n",
        "    name = re.sub(r'\\s+', ' ', name)\n",
        "    name = name.strip()\n",
        "\n",
        "    # Remove <u> tags\n",
        "    name = re.sub(r'</?u>', '', name)\n",
        "\n",
        "    # Exclude names that are too short or contain digits\n",
        "    if len(name) < 2 or bool(re.search(r'\\d', name)):\n",
        "        return None\n",
        "\n",
        "    return name\n",
        "\n",
        "def extract_names_from_text(text):\n",
        "    # Extract names from text\n",
        "    # Create an empty list to hold names\n",
        "    name_list = []\n",
        "\n",
        "    # Handle authors list separated by commas\n",
        "    if ',' in text:\n",
        "        author_list = text.split(',')\n",
        "        for author in author_list:\n",
        "            name = clean_name(author)\n",
        "            if name:\n",
        "                name_list.append(name)\n",
        "    else:\n",
        "        name = clean_name(text)\n",
        "        if name:\n",
        "            name_list.append(name)\n",
        "\n",
        "    return name_list\n",
        "\n",
        "def parse_html_for_names(html_content):\n",
        "    # Extract all researcher names from HTML\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    # Remove duplicates\n",
        "    name_list_to_set = set()\n",
        "\n",
        "    # Find all list items containing presentation titles and author information\n",
        "    for li in soup.find_all('li'):\n",
        "        text = li.get_text()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Find <i> tags containing author information\n",
        "        authors_tag = li.find('i')\n",
        "        if authors_tag:\n",
        "            authors_text = authors_tag.get_text()\n",
        "            name_list = extract_names_from_text(authors_text)\n",
        "            name_list_to_set.update(name_list)\n",
        "\n",
        "    # Find session chairs\n",
        "    chair_patterns = soup.find_all(string=re.compile(r'Chair:', re.IGNORECASE))\n",
        "    for pattern in chair_patterns:\n",
        "        chair_text = pattern.strip()\n",
        "        if 'Chair:' in chair_text:\n",
        "            chair_name = chair_text.split('Chair:')[1]\n",
        "            name = clean_name(chair_name)\n",
        "            if name:\n",
        "                name_list_to_set.add(name)\n",
        "\n",
        "    return sorted(list(name_list_to_set))\n",
        "\n",
        "def main():\n",
        "    # Set HTML file path (Google Drive path)\n",
        "    file_path = '/content/drive/MyDrive/IC2S2_2023.html'\n",
        "    # Alternatively, URL can be used directly\n",
        "\n",
        "    # Read HTML file\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    # Extract names\n",
        "    names = parse_html_for_names(html_content)\n",
        "\n",
        "    # Save results to file\n",
        "    output_path = '/content/drive/MyDrive/ic2s2_2023_researchers.txt'\n",
        "    df_path = '/content/drive/MyDrive/ic2s2_2023_researchers.csv'\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for name in names:\n",
        "            f.write(name + '\\n')\n",
        "\n",
        "    print(f\"A total of {len(names)} unique researcher names have been extracted.\")\n",
        "    print(f\"Results have been saved to {output_path} and {df_path}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "\n",
        "!pip install fuzzywuzzy[speedup]\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from fuzzywuzzy import fuzz\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def extract_person_from_candidate(candidate):\n",
        "    \"\"\"\n",
        "    Since each cell entry is in the format \"Person Name, Other Information\",\n",
        "    extract the text before the first comma as the person's name.\n",
        "    If an affiliation in parentheses is present, remove the parentheses and its content.\n",
        "\n",
        "    Example:\n",
        "      \"Chris Kempes, ...\"                  -> \"Chris Kempes\"\n",
        "      \"Chris Kempes (Santa Fe Institute)\"  -> \"Chris Kempes\"\n",
        "    \"\"\"\n",
        "    candidate = candidate.strip()\n",
        "    # If there is a comma, use the text before the first comma; otherwise, use the entire text\n",
        "    if ',' in candidate:\n",
        "        person = candidate.split(',', 1)[0]\n",
        "    else:\n",
        "        person = candidate\n",
        "    # Remove parentheses and the text within\n",
        "    person = re.sub(r'\\s*\\(.*?\\)', '', person)\n",
        "    return person.strip()\n",
        "\n",
        "def extract_names_from_cell(cell):\n",
        "    \"\"\"\n",
        "    Assumes the cell text is in the format \"Person Name, Other Information; Person Name, Other Information; ...\"\n",
        "    Splits the string by semicolons (;) and applies extract_person_from_candidate on each entry.\n",
        "    Returns only those names that consist of two or more words.\n",
        "    \"\"\"\n",
        "    names = []\n",
        "    parts = cell.split(';')\n",
        "    for part in parts:\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        person_name = extract_person_from_candidate(part)\n",
        "        if len(person_name.split()) >= 2:\n",
        "            names.append(person_name)\n",
        "    return names\n",
        "\n",
        "def extract_names_from_df(df, column_name):\n",
        "    \"\"\"\n",
        "    From the specified column (e.g., 'Poster authors' or 'Presentation authors') of the given DataFrame,\n",
        "    apply extract_names_from_cell() to extract all candidate person names.\n",
        "    \"\"\"\n",
        "    authors = []\n",
        "    if column_name in df.columns:\n",
        "        for entry in df[column_name].dropna():\n",
        "            authors.extend(extract_names_from_cell(entry))\n",
        "    else:\n",
        "        print(f\"Column '{column_name}' does not exist. Available columns: {df.columns.tolist()}\")\n",
        "    return authors\n",
        "\n",
        "def cluster_names(names, threshold=90):\n",
        "    \"\"\"\n",
        "    Uses fuzzywuzzy's token_sort_ratio to group names that have a similarity score above the threshold,\n",
        "    considering them as the same individual. Within each cluster, the shortest (cleanest) version of the name\n",
        "    is selected as the representative.\n",
        "\n",
        "    Returns:\n",
        "      representative_names: Final list of unique person names\n",
        "      clusters: List of names for each cluster (for debugging)\n",
        "    \"\"\"\n",
        "    names_list = list(set(names))\n",
        "    clusters = []\n",
        "    used = set()\n",
        "    for i, name in enumerate(names_list):\n",
        "        if name in used:\n",
        "            continue\n",
        "        cluster = [name]\n",
        "        used.add(name)\n",
        "        for other in names_list[i+1:]:\n",
        "            if other in used:\n",
        "                continue\n",
        "            score = fuzz.token_sort_ratio(name, other)\n",
        "            if score >= threshold:\n",
        "                cluster.append(other)\n",
        "                used.add(other)\n",
        "        clusters.append(cluster)\n",
        "    representative_names = [min(cluster, key=len) for cluster in clusters]\n",
        "    return representative_names, clusters\n",
        "\n",
        "def main():\n",
        "    # Set CSV file paths (using actual Google Drive paths)\n",
        "    poster_csv    = '/content/drive/MyDrive/IC2S2_2024_posters.csv'\n",
        "    lightning_csv = '/content/drive/MyDrive/IC2S2_2024_lightning_talks.csv'\n",
        "    orals_csv     = '/content/drive/MyDrive/IC2S2_2024_oral_panels.csv'\n",
        "\n",
        "    # Read CSV files\n",
        "    posters_df   = pd.read_csv(poster_csv)\n",
        "    lightning_df = pd.read_csv(lightning_csv)\n",
        "    orals_df     = pd.read_csv(orals_csv)\n",
        "\n",
        "    # Extract candidate names from the author columns of each DataFrame\n",
        "    poster_authors      = extract_names_from_df(posters_df, 'Poster authors')\n",
        "    lightning_authors   = extract_names_from_df(lightning_df, 'Presentation authors')\n",
        "    orals_authors       = extract_names_from_df(orals_df, 'Presentation authors')\n",
        "\n",
        "    # Combine the results from all three files\n",
        "    all_authors = poster_authors + lightning_authors + orals_authors\n",
        "    print(\"Total extracted candidate count (including duplicates):\", len(all_authors))\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_authors = list(set(all_authors))\n",
        "    print(\"Unique candidate count after removing duplicates:\", len(unique_authors))\n",
        "\n",
        "    # Use fuzzy matching to group slightly variant names and select representative names\n",
        "    final_names, clusters = cluster_names(unique_authors, threshold=90)\n",
        "    final_names = sorted(final_names)\n",
        "\n",
        "    # Set the output file path for the results\n",
        "    output_path = '/content/drive/MyDrive/IC2S2_2024_final_person_names_from_csv.txt'\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for name in final_names:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "    print(\"Final unique person name count:\", len(final_names))\n",
        "    print(\"Result file saved at:\", output_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "likWhFaH0mt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Aggregated Names with fuzzywuzzy\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from fuzzywuzzy import fuzz\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def extract_person_from_candidate(candidate):\n",
        "    \"\"\"\n",
        "    Since each cell entry is formatted as \"Person Name, Other Information\",\n",
        "    extract the text before the first comma as the person's name,\n",
        "    and if an affiliation in parentheses exists, remove it.\n",
        "    \"\"\"\n",
        "    candidate = candidate.strip()\n",
        "    if ',' in candidate:\n",
        "        person = candidate.split(',', 1)[0]\n",
        "    else:\n",
        "        person = candidate\n",
        "    # Remove parentheses and the text within\n",
        "    person = re.sub(r'\\s*\\(.*?\\)', '', person)\n",
        "    return person.strip()\n",
        "\n",
        "def extract_names_from_cell(cell):\n",
        "    \"\"\"\n",
        "    Assumes that the cell text is in the format \"Person Name, Other Information; Person Name, Other Information; ...\"\n",
        "    Splits the text by semicolons (;) and applies the extract_person_from_candidate function to each part.\n",
        "    \"\"\"\n",
        "    names = []\n",
        "    parts = cell.split(';')\n",
        "    for part in parts:\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        person_name = extract_person_from_candidate(part)\n",
        "        if len(person_name.split()) >= 2:  # Assume a valid person name has at least two words\n",
        "            names.append(person_name)\n",
        "    return names\n",
        "\n",
        "def extract_names_from_df(df, column_name):\n",
        "    \"\"\"\n",
        "    From the specified column (e.g., 'Poster authors' or 'Presentation authors') of the given DataFrame,\n",
        "    apply extract_names_from_cell() on each cell to extract all candidate person names.\n",
        "    \"\"\"\n",
        "    authors = []\n",
        "    if column_name in df.columns:\n",
        "        for entry in df[column_name].dropna():\n",
        "            authors.extend(extract_names_from_cell(entry))\n",
        "    else:\n",
        "        print(f\"Column '{column_name}' does not exist. Available columns: {df.columns.tolist()}\")\n",
        "    return authors\n",
        "\n",
        "def cluster_names_with_logging(names, threshold=90):\n",
        "    \"\"\"\n",
        "    Uses fuzzywuzzy to group names with a similarity score above the threshold,\n",
        "    treating them as the same individual. Within each cluster, the shortest name is selected\n",
        "    as the representative, and the merged names are logged.\n",
        "\n",
        "    Returns:\n",
        "      representative_names: Final list of unique person names.\n",
        "      clusters: List of names within each cluster (for debugging).\n",
        "      merge_log: Dictionary mapping representative names to the merged names.\n",
        "    \"\"\"\n",
        "    names_list = list(set(names))  # Remove duplicates\n",
        "    clusters = []\n",
        "    used = set()\n",
        "    merge_log = defaultdict(list)  # Log for merged names\n",
        "\n",
        "    for i, name in enumerate(names_list):\n",
        "        if name in used:\n",
        "            continue\n",
        "        cluster = [name]\n",
        "        used.add(name)\n",
        "        for other in names_list[i+1:]:\n",
        "            if other in used:\n",
        "                continue\n",
        "            score = fuzz.token_sort_ratio(name, other)\n",
        "            if score >= threshold:\n",
        "                cluster.append(other)\n",
        "                used.add(other)\n",
        "        clusters.append(cluster)\n",
        "        representative_name = min(cluster, key=len)  # Select the shortest name as the representative\n",
        "        for merged_name in cluster:\n",
        "            if merged_name != representative_name:\n",
        "                merge_log[representative_name].append(merged_name)\n",
        "\n",
        "    representative_names = [min(cluster, key=len) for cluster in clusters]\n",
        "    return representative_names, clusters, merge_log\n",
        "\n",
        "def print_merge_results(merge_log):\n",
        "    \"\"\"A function to neatly print the merge results.\"\"\"\n",
        "    print(\"\\n=== Merged Names Results ===\")\n",
        "    for representative, merged_names in merge_log.items():\n",
        "        if merged_names:  # Only print if there are merged names\n",
        "            print(f\"\\nRepresentative Name: {representative}\")\n",
        "            print(f\"Merged Names: {', '.join(merged_names)}\")\n",
        "    print(\"\\n============================\")\n",
        "\n",
        "def main():\n",
        "    # Set CSV file paths (using paths in Google Drive)\n",
        "    poster_csv = '/content/drive/MyDrive/IC2S2_2024_posters.csv'\n",
        "    lightning_csv = '/content/drive/MyDrive/IC2S2_2024_lightning_talks.csv'\n",
        "    orals_csv = '/content/drive/MyDrive/IC2S2_2024_oral_panels.csv'\n",
        "\n",
        "    # Read the CSV files\n",
        "    posters_df = pd.read_csv(poster_csv)\n",
        "    lightning_df = pd.read_csv(lightning_csv)\n",
        "    orals_df = pd.read_csv(orals_csv)\n",
        "\n",
        "    # Extract candidate person names from the author columns of each DataFrame\n",
        "    poster_authors = extract_names_from_df(posters_df, 'Poster authors')\n",
        "    lightning_authors = extract_names_from_df(lightning_df, 'Presentation authors')\n",
        "    orals_authors = extract_names_from_df(orals_df, 'Presentation authors')\n",
        "\n",
        "    # Combine the results from all three files\n",
        "    all_authors = poster_authors + lightning_authors + orals_authors\n",
        "\n",
        "    # Remove duplicates and use fuzzy matching to group similar names, logging the merges\n",
        "    final_names, clusters, merge_log = cluster_names_with_logging(all_authors, threshold=90)\n",
        "\n",
        "    # Print the merge results\n",
        "    print_merge_results(merge_log)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "VHv69Svb0olW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "\n",
        "import os\n",
        "\n",
        "# File paths for 2023 and 2024 (using the paths saved from previous code)\n",
        "file_2023 = '/content/drive/MyDrive/ic2s2_2023_researchers.txt'\n",
        "file_2024 = '/content/drive/MyDrive/IC2S2_2024_final_person_names_from_csv.txt'\n",
        "\n",
        "# Check if the files exist\n",
        "if os.path.exists(file_2023):\n",
        "    print(\"The 2023 file exists:\", file_2023)\n",
        "else:\n",
        "    print(\"The 2023 file does not exist. Please check the path:\", file_2023)\n",
        "\n",
        "if os.path.exists(file_2024):\n",
        "    print(\"The 2024 file exists:\", file_2024)\n",
        "else:\n",
        "    print(\"The 2024 file does not exist. Please check the path:\", file_2024)\n",
        "\n",
        "# Load the researcher list for 2023\n",
        "with open(file_2023, 'r', encoding='utf-8') as f:\n",
        "    names_2023 = f.read().splitlines()\n",
        "set_2023 = set(names_2023)\n",
        "\n",
        "# Load the researcher list for 2024\n",
        "with open(file_2024, 'r', encoding='utf-8') as f:\n",
        "    names_2024 = f.read().splitlines()\n",
        "set_2024 = set(names_2024)\n",
        "\n",
        "# Calculate the intersection (common names) between the two files\n",
        "common_names = set_2023.intersection(set_2024)\n",
        "\n",
        "print(\"Both IC2S2 2023 and 2024 covered\", len(common_names), \"names.\")\n",
        "print(\"Common names:\")\n",
        "for name in sorted(common_names):\n",
        "    print(name)\n",
        "\n",
        "# Save the results to a text file on Google Drive\n",
        "output_txt = '/content/drive/MyDrive/IC2S2_Common_names.txt'\n",
        "with open(output_txt, 'w', encoding='utf-8') as f:\n",
        "    for name in sorted(common_names):\n",
        "        f.write(name + \"\\n\")\n",
        "print(\"The results have been saved to a text file:\", output_txt)"
      ],
      "metadata": {
        "id": "gunn_CbS0qig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnDy360P0URv"
      },
      "source": [
        "### _How many unique researchers do you get?_\n",
        "#### We got 1484 unique researchers for our answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOcAfB430URv"
      },
      "source": [
        "### _Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words)._\n",
        "#### We first cleaned the names by removing any special character and/or unnecessary white spaces and commas, and excluded names that were too short or contained numbers. We then removed the word \"Chair\" from the session chairs whose names were saved as \"Chair: (name)\". Lastly, we added these cleaned names into a list. Using the list, we retrieved the information needed using BeautifulSoup. We then sorted the list by the names to get our final answer.\n",
        "\n",
        "\n",
        "###[edited_Jiwon Heo]\n",
        "To extract researcher names, we utilized a systematic process combining HTML parsing and CSV file analysis. Using `BeautifulSoup`, we parsed the HTML to locate relevant tags containing names. A cleaning function removed special characters, unnecessary whitespace, and invalid entries (e.g., names with numbers or too short). For session chairs, we specifically removed the word \"Chair\" from entries like \"Chair: (name).\" From CSV files, names were extracted by splitting author columns based on delimiters such as commas and semicolons.\n",
        "\n",
        "To ensure accuracy, fuzzy matching (`fuzzywuzzy`) was applied to group similar names and resolve variations caused by typos or format differences. The cleaned names were deduplicated and sorted into a final list. Quality was assessed by tracking unique name counts at each stage and manually reviewing clusters of similar names. This approach ensured comprehensive name retrieval while maintaining high data quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnUqvTBT0URw"
      },
      "source": [
        "## Part 2: Ready Made vs Custom Made Data\n",
        "\n",
        "### Centola\n",
        "#### Pros:\n",
        "#### - Flexibility, data collection in real time can be modified\n",
        "#### - Custom made data can be designed to address specific questions\n",
        "#### Cons:\n",
        "#### - Social network is something complex, researchers might accidentally introduce bias\n",
        "#### - Might not fully represent real world conditions\n",
        "\n",
        "\n",
        "### Nicolaide\n",
        "#### Pros:\n",
        "#### - Saves time and cost\n",
        "#### - Larger sample size\n",
        "#### Cons:\n",
        "#### - Quality of data might not be the best\n",
        "#### - Researchers do not have control over how the data was made\n",
        "\n",
        "### _How do you think these differences can influence the interpretation of the results in each study?_\n",
        "#### Centola allows for controlled experiments with specific manipulations while Nicolaides provide a real world view. However, Centola’s study might not be generalisable to the general population, while Nicolaide’s study may have causal correlations and uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhBgV0Qj0URw"
      },
      "source": [
        "## Part 3: Gathering Research Articles using the OpenAlex API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-3_Ryr80URx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "df = pd.read_csv('file02.csv')\n",
        "base_url = \"https://api.openalex.org/\"\n",
        "papers = []\n",
        "abstracts = []\n",
        "\n",
        "socialscience = {\n",
        "    \"Sociology\": \"https://openalex.org/C144024400\",\n",
        "    \"Psychology\": \"https://openalex.org/C15744967\",\n",
        "    \"Economics\": \"https://openalex.org/C162324750\",\n",
        "    \"Political Science\": \"https://openalex.org/C17744445\"\n",
        "}\n",
        "\n",
        "quantitative = {\n",
        "    \"Mathematics\": \"https://openalex.org/C33923547\",\n",
        "    \"Physics\": \"https://openalex.org/C121332964\",\n",
        "    \"Computer Science\": \"https://openalex.org/C41008148\"\n",
        "}\n",
        "\n",
        "def getwork(works_api_url, per_page=200):\n",
        "    works = []\n",
        "    cursor = \"*\"  # initial cursor\n",
        "    while True:\n",
        "        url = f\"{works_api_url}&per-page={per_page}&cursor={cursor}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200: break\n",
        "\n",
        "        data = response.json()\n",
        "        works.extend(data.get('results', []))\n",
        "\n",
        "        # next cursor for pagination\n",
        "        next_cursor = data.get('meta', {}).get('next_cursor')\n",
        "        if not next_cursor: break\n",
        "        cursor = next_cursor\n",
        "    return works\n",
        "\n",
        "def filtering(works):\n",
        "    filtered = []\n",
        "    for work in works:\n",
        "        if work.get('cited_by_count', 0) <= 10: continue  #more than 10 citations\n",
        "        if len(work.get('authorships', [])) >= 10: continue #fewer than 10 authors\n",
        "\n",
        "        concept_ids = [concept.get('id') for concept in work.get('concepts', [])]\n",
        "        is_SS = any(concept in socialscience.values() for concept in concept_ids)\n",
        "        is_quant = any(concept in quantitative.values() for concept in concept_ids)\n",
        "\n",
        "        if is_SS and is_quant: #works relevant to Computational Social Science  AND intersecting with a quantitative discipline\n",
        "            filtered.append(work)\n",
        "    return filtered\n",
        "\n",
        "def filtering2(id, worksurl, count): # Only if the author has between 5 and 5000 works\n",
        "    if 5 <= count <= 5000:\n",
        "        works = getwork(worksurl)\n",
        "        filtered_works = filtering(works)\n",
        "        return filtered_works\n",
        "    return []\n",
        "\n",
        "def extraction(work):\n",
        "    return {\n",
        "        'id': work.get('id'),\n",
        "        'publication_year': work.get('publication_year'),\n",
        "        'cited_by_count': work.get('cited_by_count'),\n",
        "        'author_ids': [author.get('author', {}).get('id') for author in work.get('authorships', [])],\n",
        "        'title': work.get('title'),\n",
        "        'abstract_inverted_index': work.get('abstract_inverted_index'),\n",
        "        'referenced_works': work.get('referenced_works', []),\n",
        "        'cited_by_api_url': work.get('cited_by_api_url'),\n",
        "        'related_works': work.get('related_works', [])\n",
        "    }\n",
        "\n",
        "# parallelize fetching and filtering works using joblib\n",
        "allfiltered = Parallel(n_jobs=-1)(\n",
        "    delayed(filtering2)(row['OpenAlex ID'], row['Works API URL'], row['Works Count'])\n",
        "    for _, row in df.iterrows()\n",
        ")\n",
        "\n",
        "for all in allfiltered:\n",
        "    for work in all:\n",
        "        details = extraction(work)\n",
        "\n",
        "        papers.append({\n",
        "            'id': details['id'],\n",
        "            'publication_year': details['publication_year'],\n",
        "            'cited_by_count': details['cited_by_count'],\n",
        "            'author_ids': details['author_ids'],\n",
        "            'referenced_works': details['referenced_works'],\n",
        "            'cited_by_api_url': details['cited_by_api_url'],\n",
        "            'related_works': details['related_works']\n",
        "        })\n",
        "\n",
        "        abstracts.append({\n",
        "            'id': details['id'],\n",
        "            'title': details['title'],\n",
        "            'abstract_inverted_index': details['abstract_inverted_index']\n",
        "        })\n",
        "\n",
        "papers_df = pd.DataFrame(papers)\n",
        "abstracts_df = pd.DataFrame(abstracts)\n",
        "\n",
        "papers_df.to_csv('papers.csv', index=False)\n",
        "abstracts_df.to_csv('abstracts.csv', index=False)\n",
        "print(\"Data saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeOe3srB0URx"
      },
      "source": [
        "### Data Overview and Reflection questions:\n",
        "\n",
        "### _Dataset summary. How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?_\n",
        "#### Number of works: 11230\n",
        "#### Number of unique researchers: 15199\n",
        "\n",
        "### _Efficiency in code. Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?_\n",
        "#### As suggested, I used joblib's parallel function to handle multiple requests. I also implemented the filters required (work count between 5-5000, works with more than 10 citations, works authored by fewer than 10 individuals and works related to computational social science). This allowed the code to be executed at a faster rate.\n",
        "\n",
        "### _Filtering Criteria and Dataset Relevance Reflect on the rationale behind setting specific thresholds. How do these filtering criteria contribute to the relevance of the dataset you compiled?_\n",
        "#### Work count filter: >5 work count allows us to focus on established authors, while <5000 work count removes authors who have too many work that could otherwise produce unwanted \"noise\"\n",
        "#### More than 10 citations: citations allow us to judge the influence of a work, and this filter will allow us to get datasets that has some sort of influence in the academic field\n",
        "#### Less than 10 authors per work: <10 authors would suggest that the work was collaborative and focused, as too many authors may cause a paper to have too many ideas and may not reflect clear insights\n",
        "#### Relevance to Computational Social Science: as the course is related to computational social science, it is important that the works we find are related to it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23-P_1nu0URy"
      },
      "source": [
        "### _Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?_\n",
        "#### Yes, I believe that there may be some sort of underrepresentation due to such filters.\n",
        "#### Firstly, there may be newly published work that currently have <10 citations but is still highly relevant to the field. The filter would then exclude such works which may sometimes be highly niche that results in the lower citation count. Secondly, filtering of <10 authors could potentially exclude works that were done by a large team. It could have been a complex topic that required a large team to collaborate on, which would still be relevant to the field but are excluded due to such filters."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}