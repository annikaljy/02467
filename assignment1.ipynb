{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 1\n",
    "### 02467 Computational Social Science Group 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Web-scraping\n",
    "### _Exercise: Web-scraping the list of participants to the International Conference in Computational Social Science_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _How many unique researchers do you get?_\n",
    "#### We got 1484 unique researchers for our answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words)._\n",
    "#### We first cleaned the names by removing any special character and/or unnecessary white spaces and commas, and excluded names that were too short or contained numbers. We then removed the word \"Chair\" from the session chairs whose names were saved as \"Chair: (name)\". Lastly, we added these cleaned names into a list. Using the list, we retrieved the information needed using BeautifulSoup. We then sorted the list by the names to get our final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "df = pd.read_csv('file02.csv')\n",
    "base_url = \"https://api.openalex.org/\"\n",
    "papers = []\n",
    "abstracts = []\n",
    "\n",
    "socialscience = {\n",
    "    \"Sociology\": \"https://openalex.org/C144024400\",\n",
    "    \"Psychology\": \"https://openalex.org/C15744967\",\n",
    "    \"Economics\": \"https://openalex.org/C162324750\",\n",
    "    \"Political Science\": \"https://openalex.org/C17744445\"\n",
    "}\n",
    "\n",
    "quantitative = {\n",
    "    \"Mathematics\": \"https://openalex.org/C33923547\",\n",
    "    \"Physics\": \"https://openalex.org/C121332964\",\n",
    "    \"Computer Science\": \"https://openalex.org/C41008148\"\n",
    "}\n",
    "\n",
    "def getwork(works_api_url, per_page=200):\n",
    "    works = []\n",
    "    cursor = \"*\"  # initial cursor\n",
    "    while True:\n",
    "        url = f\"{works_api_url}&per-page={per_page}&cursor={cursor}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200: break\n",
    "        \n",
    "        data = response.json()\n",
    "        works.extend(data.get('results', []))\n",
    "        \n",
    "        # next cursor for pagination\n",
    "        next_cursor = data.get('meta', {}).get('next_cursor')\n",
    "        if not next_cursor: break\n",
    "        cursor = next_cursor  \n",
    "    return works\n",
    "\n",
    "def filtering(works):\n",
    "    filtered = []\n",
    "    for work in works:\n",
    "        if work.get('cited_by_count', 0) <= 10: continue  #more than 10 citations\n",
    "        if len(work.get('authorships', [])) >= 10: continue #fewer than 10 authors\n",
    "        \n",
    "        concept_ids = [concept.get('id') for concept in work.get('concepts', [])]\n",
    "        is_SS = any(concept in socialscience.values() for concept in concept_ids)\n",
    "        is_quant = any(concept in quantitative.values() for concept in concept_ids)\n",
    "\n",
    "        if is_SS and is_quant: #works relevant to Computational Social Science  AND intersecting with a quantitative discipline\n",
    "            filtered.append(work)\n",
    "    return filtered\n",
    "\n",
    "def filtering2(id, worksurl, count): # Only if the author has between 5 and 5000 works\n",
    "    if 5 <= count <= 5000:\n",
    "        works = getwork(worksurl)\n",
    "        filtered_works = filtering(works)\n",
    "        return filtered_works\n",
    "    return []\n",
    "\n",
    "def extraction(work):\n",
    "    return {\n",
    "        'id': work.get('id'),\n",
    "        'publication_year': work.get('publication_year'),\n",
    "        'cited_by_count': work.get('cited_by_count'),\n",
    "        'author_ids': [author.get('author', {}).get('id') for author in work.get('authorships', [])],\n",
    "        'title': work.get('title'),\n",
    "        'abstract_inverted_index': work.get('abstract_inverted_index'),\n",
    "        'referenced_works': work.get('referenced_works', []), \n",
    "        'cited_by_api_url': work.get('cited_by_api_url'),\n",
    "        'related_works': work.get('related_works', []) \n",
    "    }\n",
    "\n",
    "# parallelize fetching and filtering works using joblib\n",
    "allfiltered = Parallel(n_jobs=-1)(\n",
    "    delayed(filtering2)(row['OpenAlex ID'], row['Works API URL'], row['Works Count'])\n",
    "    for _, row in df.iterrows()\n",
    ")\n",
    "\n",
    "for all in allfiltered:\n",
    "    for work in all:\n",
    "        details = extraction(work)\n",
    "    \n",
    "        papers.append({\n",
    "            'id': details['id'],\n",
    "            'publication_year': details['publication_year'],\n",
    "            'cited_by_count': details['cited_by_count'],\n",
    "            'author_ids': details['author_ids'],\n",
    "            'referenced_works': details['referenced_works'],\n",
    "            'cited_by_api_url': details['cited_by_api_url'],\n",
    "            'related_works': details['related_works']\n",
    "        })\n",
    "        \n",
    "        abstracts.append({\n",
    "            'id': details['id'],\n",
    "            'title': details['title'],\n",
    "            'abstract_inverted_index': details['abstract_inverted_index']\n",
    "        })\n",
    "\n",
    "papers_df = pd.DataFrame(papers)\n",
    "abstracts_df = pd.DataFrame(abstracts)\n",
    "\n",
    "papers_df.to_csv('papers.csv', index=False)\n",
    "abstracts_df.to_csv('abstracts.csv', index=False)\n",
    "print(\"Data saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview and Reflection questions:\n",
    "\n",
    "#### Dataset summary. How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?\n",
    "\n",
    "#### Efficiency in code. Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? (answer in max 150 words)\n",
    "\n",
    "#### Filtering Criteria and Dataset Relevance Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? (answer in max 150 words)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
