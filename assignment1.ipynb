{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Link to repository: https://github.com/annikaljy/02467/\n",
        "\n",
        "## Contributions:\n",
        "### Heo, Jiwon - Part 1\n",
        "### Law, Annika Jie Yu - Part 2 and 3\n",
        "### Byun, Jane - Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmE8x6ZI0URp"
      },
      "source": [
        "# Assigment 1\n",
        "### 02467 Computational Social Science Group 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5nPdY790URt"
      },
      "source": [
        "## Part 1: Web-scraping\n",
        "### _Exercise: Web-scraping the list of participants to the International Conference in Computational Social Science_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAwKhXU30URt"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def clean_name(name):\n",
        "    # Clean name - remove special characters and unnecessary whitespace\n",
        "    name = re.sub(r'[,\\n\\t\\r]', '', name)\n",
        "    name = re.sub(r'\\s+', ' ', name)\n",
        "    name = name.strip()\n",
        "    name = re.sub(r'</?u>', '', name)\n",
        "\n",
        "    # Exclude names that are too short or contain digits\n",
        "    if len(name) < 2 or bool(re.search(r'\\d', name)):\n",
        "        return None\n",
        "\n",
        "    return name\n",
        "\n",
        "def extract_names_from_text(text):\n",
        "    # Extract names from text\n",
        "    name_list = []\n",
        "    if ',' in text:\n",
        "        author_list = text.split(',')\n",
        "        for author in author_list:\n",
        "            name = clean_name(author)\n",
        "            if name:\n",
        "                name_list.append(name)\n",
        "    else:\n",
        "        name = clean_name(text)\n",
        "        if name:\n",
        "            name_list.append(name)\n",
        "    return name_list\n",
        "\n",
        "def parse_html_for_names(html_content):\n",
        "    # Extract all researcher names from HTML\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    name_list_to_set = set()\n",
        "\n",
        "    # Find all list items containing presentation titles and author information\n",
        "    for li in soup.find_all('li'):\n",
        "        text = li.get_text()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Find <i> tags containing author information\n",
        "        authors_tag = li.find('i')\n",
        "        if authors_tag:\n",
        "            authors_text = authors_tag.get_text()\n",
        "            name_list = extract_names_from_text(authors_text)\n",
        "            name_list_to_set.update(name_list)\n",
        "\n",
        "    # Find session chairs\n",
        "    chair_patterns = soup.find_all(string=re.compile(r'Chair:', re.IGNORECASE))\n",
        "    for pattern in chair_patterns:\n",
        "        chair_text = pattern.strip()\n",
        "        if 'Chair:' in chair_text:\n",
        "            chair_name = chair_text.split('Chair:')[1]\n",
        "            name = clean_name(chair_name)\n",
        "            if name:\n",
        "                name_list_to_set.add(name)\n",
        "\n",
        "    return sorted(list(name_list_to_set))\n",
        "\n",
        "def main():\n",
        "    # URL of the conference program\n",
        "    url = \"https://2023.ic2s2.org/program\"\n",
        "\n",
        "    # Fetch HTML content from the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        html_content = response.text\n",
        "        # Extract names\n",
        "        names = parse_html_for_names(html_content)\n",
        "\n",
        "        # Save results to a file\n",
        "        output_path = 'ic2s2_2023_researchers.txt'\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            for name in names:\n",
        "                f.write(name + '\\n')\n",
        "\n",
        "        print(f\"A total of {len(names)} unique researcher names have been extracted.\")\n",
        "        print(f\"Results have been saved to {output_path}.\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnDy360P0URv"
      },
      "source": [
        "### _How many unique researchers do you get?_\n",
        "#### We got 1484 unique researchers for our answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOcAfB430URv"
      },
      "source": [
        "### _Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words)._\n",
        "\n",
        "\n",
        "To web-scrape the conference webpage, I used the `requests` library to fetch the HTML content and `BeautifulSoup` to parse it. I inspected the webpage structure to identify relevant elements containing researcher names, such as `` tags for authors and `` tags for author details. I also searched for text patterns like \"Chair:\" to extract session chairs. Names were cleaned using regular expressions to remove unwanted characters, whitespace, and duplicates.\n",
        "\n",
        "To maximize accuracy, I handled different name formats (e.g., comma-separated lists) and ensured all names included both first and last names. I used a set to store names, avoiding duplicates caused by slight variations in formatting.\n",
        "\n",
        "To assess quality, I manually reviewed a sample of extracted names for completeness and correctness. The cleaning process ensured no invalid or partial names were included, resulting in a comprehensive and accurate list of unique contributors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnUqvTBT0URw"
      },
      "source": [
        "## Part 2: Ready Made vs Custom Made Data\n",
        "\n",
        "### Centola\n",
        "#### Pros:\n",
        "#### - Flexibility, data collection in real time can be modified\n",
        "#### - Custom made data can be designed to address specific questions\n",
        "#### Cons:\n",
        "#### - Social network is something complex, researchers might accidentally introduce bias\n",
        "#### - Might not fully represent real world conditions\n",
        "\n",
        "\n",
        "### Nicolaide\n",
        "#### Pros:\n",
        "#### - Saves time and cost\n",
        "#### - Larger sample size\n",
        "#### Cons:\n",
        "#### - Quality of data might not be the best\n",
        "#### - Researchers do not have control over how the data was made\n",
        "\n",
        "### _How do you think these differences can influence the interpretation of the results in each study?_\n",
        "#### Centola allows for controlled experiments with specific manipulations while Nicolaides provide a real world view. However, Centola’s study might not be generalisable to the general population, while Nicolaide’s study may have causal correlations and uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhBgV0Qj0URw"
      },
      "source": [
        "## Part 3: Gathering Research Articles using the OpenAlex API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-3_Ryr80URx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "df = pd.read_csv('file02.csv')\n",
        "base_url = \"https://api.openalex.org/\"\n",
        "papers = []\n",
        "abstracts = []\n",
        "\n",
        "socialscience = {\n",
        "    \"Sociology\": \"https://openalex.org/C144024400\",\n",
        "    \"Psychology\": \"https://openalex.org/C15744967\",\n",
        "    \"Economics\": \"https://openalex.org/C162324750\",\n",
        "    \"Political Science\": \"https://openalex.org/C17744445\"\n",
        "}\n",
        "\n",
        "quantitative = {\n",
        "    \"Mathematics\": \"https://openalex.org/C33923547\",\n",
        "    \"Physics\": \"https://openalex.org/C121332964\",\n",
        "    \"Computer Science\": \"https://openalex.org/C41008148\"\n",
        "}\n",
        "\n",
        "def getwork(works_api_url, per_page=200):\n",
        "    works = []\n",
        "    cursor = \"*\"  # initial cursor\n",
        "    while True:\n",
        "        url = f\"{works_api_url}&per-page={per_page}&cursor={cursor}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200: break\n",
        "\n",
        "        data = response.json()\n",
        "        works.extend(data.get('results', []))\n",
        "\n",
        "        # next cursor for pagination\n",
        "        next_cursor = data.get('meta', {}).get('next_cursor')\n",
        "        if not next_cursor: break\n",
        "        cursor = next_cursor\n",
        "    return works\n",
        "\n",
        "def filtering(works):\n",
        "    filtered = []\n",
        "    for work in works:\n",
        "        if work.get('cited_by_count', 0) <= 10: continue  #more than 10 citations\n",
        "        if len(work.get('authorships', [])) >= 10: continue #fewer than 10 authors\n",
        "\n",
        "        concept_ids = [concept.get('id') for concept in work.get('concepts', [])]\n",
        "        is_SS = any(concept in socialscience.values() for concept in concept_ids)\n",
        "        is_quant = any(concept in quantitative.values() for concept in concept_ids)\n",
        "\n",
        "        if is_SS and is_quant: #works relevant to Computational Social Science  AND intersecting with a quantitative discipline\n",
        "            filtered.append(work)\n",
        "    return filtered\n",
        "\n",
        "def filtering2(id, worksurl, count): # Only if the author has between 5 and 5000 works\n",
        "    if 5 <= count <= 5000:\n",
        "        works = getwork(worksurl)\n",
        "        filtered_works = filtering(works)\n",
        "        return filtered_works\n",
        "    return []\n",
        "\n",
        "def extraction(work):\n",
        "    return {\n",
        "        'id': work.get('id'),\n",
        "        'publication_year': work.get('publication_year'),\n",
        "        'cited_by_count': work.get('cited_by_count'),\n",
        "        'author_ids': [author.get('author', {}).get('id') for author in work.get('authorships', [])],\n",
        "        'title': work.get('title'),\n",
        "        'abstract_inverted_index': work.get('abstract_inverted_index'),\n",
        "        'referenced_works': work.get('referenced_works', []),\n",
        "        'cited_by_api_url': work.get('cited_by_api_url'),\n",
        "        'related_works': work.get('related_works', [])\n",
        "    }\n",
        "\n",
        "# parallelize fetching and filtering works using joblib\n",
        "allfiltered = Parallel(n_jobs=-1)(\n",
        "    delayed(filtering2)(row['OpenAlex ID'], row['Works API URL'], row['Works Count'])\n",
        "    for _, row in df.iterrows()\n",
        ")\n",
        "\n",
        "for all in allfiltered:\n",
        "    for work in all:\n",
        "        details = extraction(work)\n",
        "\n",
        "        papers.append({\n",
        "            'id': details['id'],\n",
        "            'publication_year': details['publication_year'],\n",
        "            'cited_by_count': details['cited_by_count'],\n",
        "            'author_ids': details['author_ids'],\n",
        "            'referenced_works': details['referenced_works'],\n",
        "            'cited_by_api_url': details['cited_by_api_url'],\n",
        "            'related_works': details['related_works']\n",
        "        })\n",
        "\n",
        "        abstracts.append({\n",
        "            'id': details['id'],\n",
        "            'title': details['title'],\n",
        "            'abstract_inverted_index': details['abstract_inverted_index']\n",
        "        })\n",
        "\n",
        "papers_df = pd.DataFrame(papers)\n",
        "abstracts_df = pd.DataFrame(abstracts)\n",
        "\n",
        "papers_df.to_csv('papers.csv', index=False)\n",
        "abstracts_df.to_csv('abstracts.csv', index=False)\n",
        "print(\"Data saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeOe3srB0URx"
      },
      "source": [
        "### Data Overview and Reflection questions:\n",
        "\n",
        "### _Dataset summary. How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?_\n",
        "#### Number of works: 11230\n",
        "#### Number of unique researchers: 15199\n",
        "\n",
        "### _Efficiency in code. Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?_\n",
        "#### As suggested, I used joblib's parallel function to handle multiple requests. I also implemented the filters required (work count between 5-5000, works with more than 10 citations, works authored by fewer than 10 individuals and works related to computational social science). This allowed the code to be executed at a faster rate.\n",
        "\n",
        "### _Filtering Criteria and Dataset Relevance Reflect on the rationale behind setting specific thresholds. How do these filtering criteria contribute to the relevance of the dataset you compiled?_\n",
        "#### Work count filter: >5 work count allows us to focus on established authors, while <5000 work count removes authors who have too many work that could otherwise produce unwanted \"noise\"\n",
        "#### More than 10 citations: citations allow us to judge the influence of a work, and this filter will allow us to get datasets that has some sort of influence in the academic field\n",
        "#### Less than 10 authors per work: <10 authors would suggest that the work was collaborative and focused, as too many authors may cause a paper to have too many ideas and may not reflect clear insights\n",
        "#### Relevance to Computational Social Science: as the course is related to computational social science, it is important that the works we find are related to it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23-P_1nu0URy"
      },
      "source": [
        "### _Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?_\n",
        "#### Yes, I believe that there may be some sort of underrepresentation due to such filters.\n",
        "#### Firstly, there may be newly published work that currently have <10 citations but is still highly relevant to the field. The filter would then exclude such works which may sometimes be highly niche that results in the lower citation count. Secondly, filtering of <10 authors could potentially exclude works that were done by a large team. It could have been a complex topic that required a large team to collaborate on, which would still be relevant to the field but are excluded due to such filters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: The Network of Computational Social Scientists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# Generate weighted edge list\n",
        "def generate_weighted_edgelist(df):\n",
        "    edge_weights = defaultdict(int)\n",
        "\n",
        "    # Convert author_ids from string to list if necessary\n",
        "    df[\"author_ids\"] = df[\"author_ids\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "    for authors in df[\"author_ids\"]:\n",
        "        if not isinstance(authors, list):\n",
        "            continue\n",
        "        \n",
        "        authors = [a.strip() for a in authors if a.strip() and a not in [\"'\", \";\", \"/\"]]\n",
        "\n",
        "        for author_pair in combinations(sorted(authors), 2):  # Unique co-author pairs\n",
        "            edge_weights[author_pair] += 1\n",
        "\n",
        "    return [(a, b, weight) for (a, b), weight in edge_weights.items()]\n",
        "\n",
        "\n",
        "\n",
        "papers_df = pd.read_csv(\"papers.csv\")\n",
        "weighted_edgelist = generate_weighted_edgelist(papers_df)\n",
        "\n",
        "# Create an undirected weighted graph\n",
        "G = nx.Graph()\n",
        "\n",
        "\n",
        "author_df = pd.read_excel(\"author_data.xlsx\")\n",
        "json_output_path = \"network_data.json\"\n",
        "\n",
        "# Rename columns to match expected names\n",
        "author_df = author_df.rename(columns={\"id\": \"Author_ID\"})\n",
        "\n",
        "# Convert to dictionary for quick lookup\n",
        "author_info = author_df.set_index(\"Author_ID\").to_dict()\n",
        "\n",
        "# Rename columns to match expected names\n",
        "papers_df = papers_df.rename(columns={\"author_ids\": \"Author_IDs\"})\n",
        "\n",
        "\n",
        "# Initialize dictionaries for citation counts and first publication years\n",
        "author_citations = {}\n",
        "first_pub_year = {}\n",
        "\n",
        "\n",
        "\n",
        "# Process each row in papers dataset\n",
        "for _, row in papers_df.iterrows():\n",
        "    publication_year = int(row[\"publication_year\"])\n",
        "    cited_by_count = row[\"cited_by_count\"]\n",
        "    author_ids = row[\"Author_IDs\"]\n",
        "    num_authors = len(author_ids)\n",
        "    \n",
        "    \n",
        "    for author_id in author_ids:\n",
        "        # Update total citation count per author\n",
        "        if author_id in author_citations:\n",
        "            author_citations[author_id] += cited_by_count\n",
        "        else:\n",
        "            author_citations[author_id] = cited_by_count\n",
        "        \n",
        "        # Update first publication year per author\n",
        "        if author_id in first_pub_year:\n",
        "            first_pub_year[author_id] = min(first_pub_year[author_id], publication_year)\n",
        "        else:\n",
        "            first_pub_year[author_id] = publication_year\n",
        "   \n",
        "\n",
        "# Create graph and add nodes with attributes\n",
        "\n",
        "for _, row in author_df.iterrows():\n",
        "    author_id = row[\"Author_ID\"]\n",
        "    G.add_node(\n",
        "        author_id,\n",
        "        display_name=row[\"display_name\"],\n",
        "        country=row[\"country_code\"],\n",
        "        first_publication_year=first_pub_year.get(author_id, None),  # Default to None if no data\n",
        "        citation_count=author_citations.get(author_id, 0),  # Default to 0 if no citations\n",
        "    )\n",
        "\n",
        "G.add_weighted_edges_from(weighted_edgelist)\n",
        "\n",
        "\n",
        "# Save network as JSON\n",
        "network_json = nx.node_link_data(G, edges=\"links\") \n",
        "with open(json_output_path, \"w\") as f:\n",
        "    json.dump(network_json, f, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "# Return the JSON file path\n",
        "print(json_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Preliminary Network Analysis Now, with the network constructed, perform a basic analysis to explore its features.\n",
        "\n",
        "### 1. Network Metrics:\n",
        "What is the total number of nodes (authors) and links (collaborations) in the network?\n",
        "Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.\n",
        "Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?\n",
        "If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset.\n",
        "How many isolated nodes are there in your network? An isolated node is defined as a node with no connections to any other node in the network.\n",
        "Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why? (answer in max 150 words)\n",
        "\n",
        "Answer : The network has 15,691 authors and 48,354 collaborations, with a low density of 0.00039, indicating a highly sparse structure. It consists of 743 disconnected components, with the largest component containing 12,158 nodes. Additionally, 500 authors are isolated, meaning they have no collaborations.\n",
        "These findings suggest that researchers mainly collaborate within specific groups rather than across the entire network. The presence of multiple disconnected components aligns with typical academic collaboration patterns, where researchers tend to work within specialized communities.\n",
        "\n",
        "### 2. Degree Analysis:\n",
        "\n",
        "Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree). What do these metrics tell us about the network? (answer in max 150 words)\n",
        "\n",
        "Answer : The average degree is 6.16, meaning authors typically collaborate with about 6 researchers. The median degree is 5, and the mode is 4, indicating that most authors have only a few co-authors. However, the most connected author has 362 collaborators.\n",
        "The highest node strength (weighted degree) is 1,078, suggesting that some authors have collaborated multiple times with many others. This reveals that a few hub researchers drive a large portion of the collaborations, while most researchers have limited connections.\n",
        "\n",
        "### 3. Top Authors:\n",
        "\n",
        "Identify the top 5 authors by degree. What role do these node play in the network?\n",
        "Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? (answer in max 150 words)\n",
        "\n",
        "Answer : The top 5 authors by degree are Yan Wang (362), Yi Yang (306), Alex Pentland (263), Robert West (255), and Simon A. Levin (240). Alex Pentland, Robert West, and Simon A. Levin are well-known for their work in network science, data science, and computational social science.\n",
        "However, Yan Wang and Yi Yang may specialize in broader fields such as artificial intelligence or computer science, explaining their high degree. Their interdisciplinary work likely contributes to Computational Social Science through data-driven research and network analysis."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
